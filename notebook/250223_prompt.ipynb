{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(303)\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/doyooni303/experiments/LLMRec/data/amazon/Books\"\n",
    "fname = \"Books\"\n",
    "meta_name_dict = json.load(\n",
    "    open(os.path.join(data_folder, f\"{fname}_meta_name_dict.json\"), \"r\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Get Untamed: The Journal (How to Quit Pleasing and Start Living)'),\n",
       " ('2',\n",
       "  'Cute & Easy Crochet with Flowers: 35 beautiful projects using floral motifs'),\n",
       " ('3', 'The Pop Manga Sketchbook: A Guided Drawing Journal'),\n",
       " ('4',\n",
       "  'Haiku Knits: 25 Serenely Beautiful Patterns Inspired by Japanese Design')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(meta_name_dict['title'].items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_users = json.load(\n",
    "    open(os.path.join(data_folder, f\"{fname}_similar_users.json\"), \"r\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547534"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_users = list(similar_users.keys()); len(sim_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 1188598/1188598 [00:02<00:00, 403198.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/doyooni303/experiments/LLMRec/ReLLMRec/src/dataset\")\n",
    "from utils import data_partition\n",
    "\n",
    "train, valid, test = data_partition(fname, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928205"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/doyooni303/experiments/LLMRec/ReLLMRec/src/dataset\")\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from utils import data_partition\n",
    "\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, config, flag:str = None) -> None:\n",
    "        self.config = config\n",
    "        self.path = config['path']\n",
    "        self.fname = config['fname']\n",
    "        self.max_items = config['max_items']\n",
    "        self.min_items = config['min_items']\n",
    "        self.flag = flag\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self._load_data()\n",
    "        \n",
    "\n",
    "    def _load_data(self,):\n",
    "        def _load_json(path:str, name: str)-> Dict:\n",
    "            return json.load(open(os.path.join(path, name), \"r\"))\n",
    "\n",
    "        def _get_data(fname: str, path: str, min_items: int, flag: str):\n",
    "            train, valid, test = data_partition(fname, path, min_items)\n",
    "            if flag == \"train\":\n",
    "                data = train\n",
    "            else:\n",
    "                data = train\n",
    "                for user_id in data.keys():\n",
    "                    if flag == \"valid\":\n",
    "                        data[user_id].append(valid[user_id])\n",
    "                    elif flag == \"test\":\n",
    "                        data[user_id].extend([valid[user_id], test[user_id]])\n",
    "                \n",
    "            return data\n",
    "\n",
    "        self.meta_name_dict = _load_json(self.path, f\"{self.fname}_meta_name_dict.json\")\n",
    "        self.similar_users = _load_json(self.path, f\"{self.fname}_similar_users.json\")\n",
    "        self.data = _get_data(self.fname, self.path, self.min_items, self.flag)\n",
    "        self.usermap = {i:user_id for i, user_id in enumerate(self.data.keys())}\n",
    "\n",
    "    def _get_item_metadata(self, meta_name_dict: dict, item_id: int) -> Dict:\n",
    "        \"\"\"Get metadata of the item\"\"\"\n",
    "        return {key: meta_name_dict[key][str(item_id)] for key in meta_name_dict.keys()}\n",
    "    \n",
    "    def _get_item_text(self,item_id: int, metadata: dict) -> str:\n",
    "            item_text = f\"<Item {item_id}>\\n\"\n",
    "            item_text += \"\".join([f\"{key}: {value}\\n\" for key,value in metadata.items()])\n",
    "            return item_text\n",
    "\n",
    "    def _format_item_list_query(self, user_id: str, data: dict, meta_name_dict: dict) -> str:\n",
    "        \"\"\"Get the item list query for the user\"\"\"\n",
    "        query = f\"Here is a item list of <user {user_id}> that shows the preference of <user {user_id}> in a time-order, so the last item is the most recent item.\\n\"\n",
    "        query += \"[Item List]\\n\"\n",
    "        # user_id = str(user_id)\n",
    "        history_items = data[user_id][-(self.max_items+1):-1]\n",
    "        for i,item_id in enumerate(history_items):\n",
    "            metadata = self._get_item_metadata(meta_name_dict, item_id)        \n",
    "            query += \"\\n\".join([f\"{i}.\\n{self._get_item_text(item_id, metadata)}\"])\n",
    "        \n",
    "        query += f\"Please select the most related items based on the <user {user_id}>'s history item list.\"\n",
    "        return query\n",
    "    \n",
    "    def _format_similar_users_query(self, user_id: str, data: dict, meta_name_dict: dict, similar_users: dict) -> str:\n",
    "        \"\"\"Get the similar users query for the user\"\"\"\n",
    "        query = f\"Here is a list of similar users of <user {user_id}> and their history item list. Items are represented with a title and the first user is the most similar user to <user {user_id}>.\\n\"\n",
    "        query += \"[Similar Users]\\n\"\n",
    "        \n",
    "        similar_user_list = [l[0] for l in similar_users[str(user_id)]]\n",
    "        for i, sim_user_id in enumerate(similar_user_list):\n",
    "            title_list = [meta_name_dict['title'][str(item_id)] for item_id in data[sim_user_id][-(self.max_items+1):]]\n",
    "            query += f\"{i}. <useer {sim_user_id}: {title_list}\\n\"\n",
    "        \n",
    "        query += f\"Please select the most related items based on the similar users of <user {user_id}>.\"\n",
    "        return query\n",
    "    \n",
    "    def __len__(self,) -> int:\n",
    "         return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple:\n",
    "        user_id = self.usermap[idx]\n",
    "        user_item_query = self._format_item_list_query(user_id, self.data, self.meta_name_dict)\n",
    "        similar_users_query = self._format_similar_users_query(user_id, self.data, self.meta_name_dict, self.similar_users)\n",
    "         \n",
    "        target_item_id = self.data[user_id][-1]\n",
    "        target_item_title = self.meta_name_dict['title'][str(target_item_id)]\n",
    "\n",
    "        tokenized_ui_query = self.tokenizer(user_item_query, return_tensors=\"pt\", padding=\"max_length\", max_length=self.config['max_length'], truncation=True)\n",
    "        tokenized_su_query = self.tokenizer(similar_users_query, return_tensors=\"pt\", padding=\"max_length\", max_length=self.config['max_length'], truncation=True)\n",
    "\n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'ui_query_input_ids': tokenized_ui_query['input_ids'].squeeze(),\n",
    "            'ui_query_attention_mask': tokenized_ui_query['attention_mask'].squeeze(),\n",
    "            'su_query_input_ids': tokenized_su_query['input_ids'].squeeze(),\n",
    "            'su_query_attention_mask': tokenized_su_query['attention_mask'].squeeze(),\n",
    "            'target_item_id': target_item_id,\n",
    "            'target_item_title': target_item_title\n",
    "        }\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'path': \"/home/doyooni303/experiments/LLMRec/data/amazon/Books\",\n",
    "    'fname': \"Books\",\n",
    "    'max_items': 15,\n",
    "    'min_items': 5,\n",
    "    'model_name': \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    'max_length': 4096,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 1188598/1188598 [00:03<00:00, 338983.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_id': tensor([393856,  71642, 355763,  48404]),\n",
       " 'ui_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ...,     17,    198,  20922]]),\n",
       " 'ui_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'su_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009]]),\n",
       " 'su_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'target_item_id': tensor([ 91337,  85772, 356151,  41431]),\n",
       " 'target_item_title': ['The Absolute Best Dump Cake Cookbook: More Than 60 Tasty Dump Cakes',\n",
       "  \"That's Not My Monster...(Usborne Touchy-Feely Books)\",\n",
       "  'DK Workbooks: Spelling, Second Grade: Learn and Explore',\n",
       "  'Ghosts - John Milton #4 (John Milton Series)']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = AmazonDataset(config, flag=\"train\")\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 1188598/1188598 [00:04<00:00, 280163.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_id': tensor([461387, 110853, 218861, 422622]),\n",
       " 'ui_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009]]),\n",
       " 'ui_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'su_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009]]),\n",
       " 'su_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'target_item_id': tensor([ 29515,  70756, 163281, 747155]),\n",
       " 'target_item_title': ['iPhone: The Missing Manual',\n",
       "  'I Love You, Little Pookie',\n",
       "  'Black & Decker The Complete Guide to Greenhouses & Garden Projects: Greenhouses, Cold Frames, Compost Bins, Trellises, Planting Beds, Potting Benches & More (Black & Decker Complete Guide)',\n",
       "  'Hoosiers Through and Through']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validset = AmazonDataset(config, flag=\"valid\")\n",
    "validloader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "next(iter(validloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
