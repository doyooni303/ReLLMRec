{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_jsonl(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [json.loads(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f8c6325f520>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/doyooni303/experiments/sequential_recsys/datasets/amazon\"\n",
    "dataset_name = \"Books\"\n",
    "history = json.load(open(os.path.join(dataset_path,dataset_name,f\"{dataset_name}_history.json\"),'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m itemidx2meta \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path,dataset_name,\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m_itemidx2meta.json\u001b[39m\u001b[39m\"\u001b[39m),\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "itemidx2meta = json.load(open(os.path.join(dataset_path,dataset_name,f\"{dataset_name}_itemidx2meta.json\"),'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longformer로 우선 아이템 임베딩 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item text부터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(meta: dict, keys: list):\n",
    "    text = \"\"\n",
    "    for key in keys:\n",
    "        try:\n",
    "            if isinstance(meta[key],str):\n",
    "                text += f\"{key}: {meta[key]}\\n\"\n",
    "            elif isinstance(meta[key],list):\n",
    "                if len(meta[key])>0:\n",
    "                    add = \", \".join(meta[key])\n",
    "                    text += f\"{key}: {add}\\n\"\n",
    "            \n",
    "            if key=='author':\n",
    "                text += f\"author: {meta['author']['name']}\\n\"\n",
    "        except:\n",
    "            text += f\"{key}: None\\n\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_category: Books\n",
      "title: Notes from a Kidwatcher\n",
      "subtitle: First Edition\n",
      "author: Yetta M. Goodman\n",
      "features: Contains 23 selected articles by this influential writer, researcher, educator, and speaker. They're grouped around six major themes inherent in teacher education: culture and community; miscue analysis, reading strategies and comprehension; print awareness and the roots of literacy; the writing process; kidwatching; and whole language theory. No index. Annotation c. by Book News, Inc., Portland, Or.\n",
      "description: About the Author, SANDRA WILDE, Ph.D., is widely recognized for her expertise in developmental spelling and her advocacy of holistic approaches to spelling and phonics. She is Professor of Curriculum and Instruction at Portland State University in Oregon. She is best known for her work in invented spelling, phonics and miscue analysis. She specializes in showing teachers how kids' invented spellings and miscues can help us work with them in more sophisticated and learner-centered ways. Looking at what kids do as they read and write is at the heart of Sandra's presentations and workshops. She can do lively keynote presentations that highlight the interesting things that we can learn by paying close attention to students' invented spellings and miscues, as well as workshops of varying lengths that focus on student-centered teaching of spelling and phonics. She has recently begun offering workshops that focus on understanding students' miscues as a guide to appropriate instruction, p\n",
      "categories: Books, Reference, Words, Language & Grammar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta = itemidx2meta['1']\n",
    "meta_keys= ['main_category',\n",
    " 'title',\n",
    " 'subtitle',\n",
    " 'author',\n",
    " 'average_rating',\n",
    " 'features',\n",
    " 'description',\n",
    " 'categories',]\n",
    "\n",
    "text = get_text(meta, meta_keys)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4448181/4448181 [00:42<00:00, 105369.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_jsonl(data: list, path: str, desc) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(data, desc=desc):\n",
    "            json.dump(line, f, ensure_ascii=False)\n",
    "            f.writelines(\"\\n\")\n",
    "\n",
    "item_texts = []\n",
    "for idx, meta in tqdm(itemidx2meta.items(), total=len(itemidx2meta)):\n",
    "    item_dict = {}\n",
    "    text = get_text(meta, meta_keys)\n",
    "    item_dict['idx'] = idx\n",
    "    item_dict['text'] = text\n",
    "    item_texts.append(item_dict)\n",
    "\n",
    "# save_jsonl(item_texts,\n",
    "#            os.path.join(dataset_path,dataset_name,f\"{dataset_name}_item_texts.jsonl\"),\n",
    "#            desc=\"Saving item texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "\n",
    "class SimilaritySearch:\n",
    "    def __init__(self, embedding_dim: int = 768):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = None\n",
    "        self.item_id_map = None\n",
    "    \n",
    "    def build_index(self, embedding_system, custom_ids=None, device='cpu'):\n",
    "        \"\"\"Build FAISS index from embedding layer\"\"\"\n",
    "        # Get all embeddings from the embedding layer\n",
    "        with torch.no_grad():  # We don't need gradients for index building\n",
    "            embeddings = embedding_system.item_embeddings.weight.data\n",
    "            if device == 'cpu':\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "            else:\n",
    "                embeddings = embeddings.cuda().numpy()\n",
    "        \n",
    "        # Normalize embeddings (optional, but recommended for cosine similarity)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Set up custom ID mapping if provided\n",
    "        if custom_ids is not None:\n",
    "            if len(custom_ids) != len(embeddings):\n",
    "                raise ValueError(f\"Length of custom_ids ({len(custom_ids)}) must match number of embeddings ({len(embeddings)})\")\n",
    "            self.item_id_map = {i: custom_id for i, custom_id in enumerate(custom_ids)}\n",
    "        else:\n",
    "            self.item_id_map = {i: i for i in range(len(embeddings))}\n",
    "        \n",
    "        # Create mapping from index position to item_id\n",
    "        # This can be customized based on your item IDs\n",
    "        self.item_id_map = None  # Will be set when custom IDs are provided\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def update_specific_embeddings(self, embeddings: torch.Tensor, indices: List[int]):\n",
    "        \"\"\"\n",
    "        Update specific embeddings in the FAISS index\n",
    "        Args:\n",
    "            embeddings: tensor of shape (n, embedding_dim) containing updated embeddings\n",
    "            indices: list of indices corresponding to the embeddings to update\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built yet! Call build_index first.\")\n",
    "            \n",
    "        # Convert embeddings to numpy\n",
    "        updated_embeddings = embeddings.detach().cpu().numpy()\n",
    "        \n",
    "        # Normalize the updated embeddings\n",
    "        faiss.normalize_L2(updated_embeddings)\n",
    "        \n",
    "        # Remove old vectors\n",
    "        self.index.remove_ids(np.array(indices))\n",
    "        \n",
    "        # Add updated vectors\n",
    "        self.index.add_with_ids(updated_embeddings, np.array(indices))\n",
    "        \n",
    "    def search(self, query_embedding: torch.Tensor, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Search for k most similar items\n",
    "        Args:\n",
    "            query_embedding: tensor of shape (1, embedding_dim)\n",
    "            k: number of similar items to retrieve\n",
    "        Returns:\n",
    "            distances, item_indices\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built yet! Call build_index first.\")\n",
    "            \n",
    "        # Convert query to numpy and ensure correct shape\n",
    "        query = query_embedding.detach().cpu().numpy()\n",
    "        if len(query.shape) == 1:\n",
    "            query = query.reshape(1, -1)\n",
    "            \n",
    "        # Normalize query (if you normalized the index)\n",
    "        faiss.normalize_L2(query)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        \n",
    "        # Map indices to original item ids\n",
    "        item_indices = np.array([self.item_id_map[idx] for idx in indices[0]])\n",
    "        \n",
    "        return distances[0], item_indices\n",
    "\n",
    "def example_usage():\n",
    "    # Assuming you have your embedding system ready\n",
    "    embedding_system = ItemEmbeddingSystem(num_items=747800)\n",
    "    \n",
    "    # Build similarity search index\n",
    "    similarity_search = SimilaritySearch(embedding_dim=768)\n",
    "    similarity_search.build_index(embedding_system)\n",
    "    \n",
    "    # Example: Get embedding for a query item\n",
    "    query_idx = torch.tensor([42])  # example item id\n",
    "    query_embedding = embedding_system(query_idx)\n",
    "    \n",
    "    # Find similar items\n",
    "    distances, similar_item_indices = similarity_search.search(query_embedding[0], k=10)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Query item:\", query_idx.item())\n",
    "    print(\"Similar items (id, distance):\")\n",
    "    for idx, dist in zip(similar_item_indices, distances):\n",
    "        print(f\"Item {idx}: {dist:.4f}\")\n",
    "    \n",
    "    return similar_item_indices, distances\n",
    "\n",
    "# Usage during training\n",
    "class RecommenderWithFAISS:\n",
    "    def __init__(self, embedding_system, rebuild_index_frequency=100):\n",
    "        self.embedding_system = embedding_system\n",
    "        self.similarity_search = SimilaritySearch(embedding_dim=768)\n",
    "        self.rebuild_index_frequency = rebuild_index_frequency\n",
    "        self.training_step = 0\n",
    "        \n",
    "    def update_and_search(self, query_idx: int, k: int = 5):\n",
    "        \"\"\"Find similar items and update their embeddings after training\"\"\"\n",
    "        # 1. Get initial similar items\n",
    "        similar_items, distances = self.find_similar_items(query_idx, k)\n",
    "        \n",
    "        # 2. Get embeddings for these items\n",
    "        item_indices = torch.tensor(similar_items)\n",
    "        original_embeddings = self.embedding_system(item_indices)\n",
    "        \n",
    "        # 3. Perform training step (example with dummy labels)\n",
    "        self.embedding_system.train()\n",
    "        loss = self.train_step(item_indices, torch.zeros_like(item_indices))\n",
    "        \n",
    "        # 4. Get updated embeddings\n",
    "        self.embedding_system.eval()\n",
    "        with torch.no_grad():\n",
    "            updated_embeddings = self.embedding_system(item_indices)\n",
    "        \n",
    "        # 5. Update these specific embeddings in FAISS\n",
    "        self.similarity_search.update_specific_embeddings(\n",
    "            updated_embeddings,\n",
    "            similar_items.tolist()\n",
    "        )\n",
    "        \n",
    "        return similar_items, original_embeddings, updated_embeddings, loss\n",
    "\n",
    "    def train_step(self, item_indices, labels):\n",
    "        # Regular training step\n",
    "        self.embedding_system.train()\n",
    "        embeddings = self.embedding_system(item_indices)\n",
    "        loss = self.calculate_loss(embeddings, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Rebuild FAISS index periodically\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.rebuild_index_frequency == 0:\n",
    "            self.embedding_system.eval()\n",
    "            self.similarity_search.build_index(self.embedding_system)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def find_similar_items(self, query_idx: int, k: int = 10):\n",
    "        \"\"\"Find similar items using FAISS\"\"\"\n",
    "        self.embedding_system.eval()\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.embedding_system(torch.tensor([query_idx]))\n",
    "        \n",
    "        return self.similarity_search.search(query_embedding[0], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is on cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  0%|          | 0/370682 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.11 GiB. GPU 3 has a total capacity of 79.15 GiB of which 1.47 GiB is free. Process 2088314 has 77.65 GiB memory in use. Of the allocated memory 73.79 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m longformer\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlongformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     20\u001b[0m item_idxs\u001b[38;5;241m.\u001b[39mextend(idxs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1729\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1721\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1722\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[1;32m   1723\u001b[0m ]\n\u001b[1;32m   1725\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1726\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m   1727\u001b[0m )\n\u001b[0;32m-> 1729\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1738\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1739\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1309\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1298\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1299\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1300\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         output_attentions,\n\u001b[1;32m   1307\u001b[0m     )\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1309\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1237\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ):\n\u001b[0;32m-> 1237\u001b[0m     self_attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1247\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1173\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1172\u001b[0m ):\n\u001b[0;32m-> 1173\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m   1183\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:562\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m query_vectors \u001b[38;5;241m=\u001b[39m query_vectors\u001b[38;5;241m.\u001b[39mview(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    560\u001b[0m key_vectors \u001b[38;5;241m=\u001b[39m key_vectors\u001b[38;5;241m.\u001b[39mview(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 562\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliding_chunks_query_key_matmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_sided_attn_window_size\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# values to pad for attention probs\u001b[39;00m\n\u001b[1;32m    567\u001b[0m remove_from_windowed_attention_mask \u001b[38;5;241m=\u001b[39m (attention_mask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:834\u001b[0m, in \u001b[0;36mLongformerSelfAttention._sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    828\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk(key, window_overlap, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx_export\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# matrix multiplication\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;66;03m# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m diagonal_chunked_attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbcxd,bcyd->bcxy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# multiply\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# convert diagonals into columns\u001b[39;00m\n\u001b[1;32m    837\u001b[0m diagonal_chunked_attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_and_transpose_last_two_dims(\n\u001b[1;32m    838\u001b[0m     diagonal_chunked_attention_scores, padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    839\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/functional.py:381\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    378\u001b[0m     _operands \u001b[38;5;241m=\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_operands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.11 GiB. GPU 3 has a total capacity of 79.15 GiB of which 1.47 GiB is free. Process 2088314 has 77.65 GiB memory in use. Of the allocated memory 73.79 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "### longformer 이용하는 경우\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is on {device}\")\n",
    "longformer = AutoModel.from_pretrained(\"allenai/longformer-base-4096\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "batch_size = 12\n",
    "item_embeddings= []\n",
    "item_idxs = []\n",
    "for i in tqdm(range(0, len(item_texts), batch_size)):\n",
    "    batches = item_texts[i:i+batch_size]\n",
    "    idxs = [meta['idx'] for meta in batches]\n",
    "    texts = [meta['text'] for meta in batches]\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", max_length=4096, truncation=True).to(device)\n",
    "    longformer.eval()\n",
    "    outputs = longformer(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:,0,:].detach().cpu().tolist()\n",
    "    item_idxs.extend(idxs)\n",
    "    item_embeddings.extend(embeddings)\n",
    "    del inputs, outputs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 17894,  1215,  ...,     1,     1,     1],\n",
       "        [    0, 17894,  1215,  ...,     1,     1,     1],\n",
       "        [    0, 17894,  1215,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0, 17894,  1215,  ...,     1,     1,     1],\n",
       "        [    0, 17894,  1215,  ...,     1,     1,     1],\n",
       "        [    0, 17894,  1215,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is on {device}\")\n",
    "longformer = AutoModel.from_pretrained(\"allenai/longformer-base-4096\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "max_length = 384\n",
    "model_kwargs = {'device': device}\n",
    "encode_kwargs = {'padding': 'max_length', 'max_length':max_length, 'return_tensors': 'pt', 'truncation': True}\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L12-v2',\n",
    "                                        model_kwargs=model_kwargs,\n",
    "                                        encode_kwargs=encode_kwargs,\n",
    "                                        )\n",
    "loader = JSONLoader(file_path=os.path.join(dataset_path,dataset_name,f\"{dataset_name}_item_texts.jsonl\"),\n",
    "                    jq_schema=\".text\",\n",
    "                    text_content=False,\n",
    "                    json_lines=True)\n",
    "data = loader.load()\n",
    "\n",
    "# index=faiss.IndexFlatL2(embeddings.)\n",
    "# index\n",
    "# db = FAISS(\n",
    "#     embedding_function=embeddings,\n",
    "#     index=\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1259765625,\n",
       " -0.26171875,\n",
       " 0.0947265625,\n",
       " 0.04638671875,\n",
       " 0.345703125,\n",
       " -0.205078125,\n",
       " -0.50390625,\n",
       " -0.443359375,\n",
       " -0.115234375,\n",
       " -0.25,\n",
       " -0.353515625,\n",
       " -0.185546875,\n",
       " 0.1953125,\n",
       " -0.34375,\n",
       " 0.095703125,\n",
       " -0.1865234375,\n",
       " -0.365234375,\n",
       " -0.0888671875,\n",
       " -0.1005859375,\n",
       " -0.0400390625,\n",
       " -0.0252685546875,\n",
       " -0.015869140625,\n",
       " -0.2451171875,\n",
       " -0.0015716552734375,\n",
       " -0.4140625,\n",
       " -0.0068359375,\n",
       " -0.1728515625,\n",
       " -0.244140625,\n",
       " 0.234375,\n",
       " 0.2451171875,\n",
       " -0.06982421875,\n",
       " -0.003204345703125,\n",
       " -0.3359375,\n",
       " 0.006500244140625,\n",
       " -0.0869140625,\n",
       " 0.2490234375,\n",
       " -0.17578125,\n",
       " 0.1611328125,\n",
       " -0.396484375,\n",
       " 0.2470703125,\n",
       " 0.02001953125,\n",
       " 0.2294921875,\n",
       " 0.263671875,\n",
       " 0.0186767578125,\n",
       " 0.1669921875,\n",
       " 0.17578125,\n",
       " -0.2119140625,\n",
       " -0.310546875,\n",
       " -0.263671875,\n",
       " 0.474609375,\n",
       " 0.291015625,\n",
       " 0.404296875,\n",
       " 0.29296875,\n",
       " 0.1005859375,\n",
       " -0.251953125,\n",
       " -0.1591796875,\n",
       " -0.06884765625,\n",
       " 0.0703125,\n",
       " -0.1552734375,\n",
       " -0.1787109375,\n",
       " -0.28125,\n",
       " -0.2490234375,\n",
       " -0.062255859375,\n",
       " -0.2021484375,\n",
       " -0.0712890625,\n",
       " 0.150390625,\n",
       " 0.08056640625,\n",
       " 0.0279541015625,\n",
       " 0.2734375,\n",
       " -0.37109375,\n",
       " -0.302734375,\n",
       " 0.11328125,\n",
       " 0.357421875,\n",
       " 0.15234375,\n",
       " 0.08544921875,\n",
       " 0.1396484375,\n",
       " -0.11328125,\n",
       " 0.1259765625,\n",
       " -0.04443359375,\n",
       " 0.447265625,\n",
       " -0.447265625,\n",
       " 0.51953125,\n",
       " 0.2021484375,\n",
       " 0.042724609375,\n",
       " -0.53125,\n",
       " 0.271484375,\n",
       " -0.41015625,\n",
       " 0.3984375,\n",
       " -0.16796875,\n",
       " -0.036376953125,\n",
       " 0.2353515625,\n",
       " 0.39453125,\n",
       " 0.0654296875,\n",
       " -0.06591796875,\n",
       " -0.1142578125,\n",
       " -0.0186767578125,\n",
       " -0.4296875,\n",
       " -0.1533203125,\n",
       " -0.490234375,\n",
       " -0.65625,\n",
       " 0.1337890625,\n",
       " -0.234375,\n",
       " -0.3515625,\n",
       " 0.384765625,\n",
       " -0.08154296875,\n",
       " -0.58203125,\n",
       " -0.361328125,\n",
       " -0.12255859375,\n",
       " 0.06787109375,\n",
       " 0.059814453125,\n",
       " 0.376953125,\n",
       " -0.04736328125,\n",
       " -0.2197265625,\n",
       " -0.03955078125,\n",
       " -0.283203125,\n",
       " 0.1787109375,\n",
       " -0.12890625,\n",
       " 0.009521484375,\n",
       " 0.015625,\n",
       " 0.1796875,\n",
       " 0.12255859375,\n",
       " 0.1630859375,\n",
       " 0.01397705078125,\n",
       " 0.08935546875,\n",
       " 0.2109375,\n",
       " 0.1279296875,\n",
       " 0.21875,\n",
       " -0.1083984375,\n",
       " 0.43359375,\n",
       " -0.07421875,\n",
       " -0.376953125,\n",
       " -0.007720947265625,\n",
       " -0.1103515625,\n",
       " -0.4453125,\n",
       " 0.2490234375,\n",
       " -0.1396484375,\n",
       " 0.34375,\n",
       " 0.044189453125,\n",
       " -0.09326171875,\n",
       " 0.23046875,\n",
       " 0.140625,\n",
       " 0.150390625,\n",
       " -0.1279296875,\n",
       " -0.0615234375,\n",
       " -0.1572265625,\n",
       " -0.078125,\n",
       " 0.26953125,\n",
       " -0.10498046875,\n",
       " 0.09814453125,\n",
       " 0.08642578125,\n",
       " -0.06103515625,\n",
       " -0.052978515625,\n",
       " 0.3125,\n",
       " -0.0247802734375,\n",
       " 0.0859375,\n",
       " -0.13671875,\n",
       " -0.064453125,\n",
       " 0.037109375,\n",
       " -0.12890625,\n",
       " -0.06591796875,\n",
       " 0.1796875,\n",
       " 0.36328125,\n",
       " 0.126953125,\n",
       " -0.08203125,\n",
       " 0.18359375,\n",
       " -0.306640625,\n",
       " -0.1923828125,\n",
       " -0.162109375,\n",
       " -0.0537109375,\n",
       " 0.21484375,\n",
       " -0.0537109375,\n",
       " 0.357421875,\n",
       " -0.2001953125,\n",
       " 0.4140625,\n",
       " 0.4453125,\n",
       " 0.00732421875,\n",
       " -0.10009765625,\n",
       " -0.16796875,\n",
       " -0.1826171875,\n",
       " 0.302734375,\n",
       " 0.11962890625,\n",
       " -0.125,\n",
       " -0.1328125,\n",
       " 0.23046875,\n",
       " 0.04345703125,\n",
       " 0.15625,\n",
       " 0.11279296875,\n",
       " -0.2109375,\n",
       " -0.045166015625,\n",
       " 0.28125,\n",
       " 0.0023345947265625,\n",
       " 0.158203125,\n",
       " -0.10009765625,\n",
       " -0.53125,\n",
       " 0.44921875,\n",
       " -0.1044921875,\n",
       " -0.158203125,\n",
       " -0.1630859375,\n",
       " 0.052001953125,\n",
       " 0.33984375,\n",
       " 0.2890625,\n",
       " -0.46875,\n",
       " 0.10205078125,\n",
       " -0.01348876953125,\n",
       " -0.53515625,\n",
       " -0.1669921875,\n",
       " -0.08154296875,\n",
       " -0.019775390625,\n",
       " -0.00653076171875,\n",
       " -0.27734375,\n",
       " 0.2216796875,\n",
       " 0.392578125,\n",
       " 0.28125,\n",
       " -0.1650390625,\n",
       " 0.2041015625,\n",
       " 0.08935546875,\n",
       " 0.392578125,\n",
       " 0.380859375,\n",
       " 0.296875,\n",
       " -0.2353515625,\n",
       " 0.2734375,\n",
       " -0.421875,\n",
       " 0.0634765625,\n",
       " -0.09375,\n",
       " 0.166015625,\n",
       " 0.263671875,\n",
       " -0.12451171875,\n",
       " 0.3203125,\n",
       " -0.05126953125,\n",
       " 0.423828125,\n",
       " 0.1689453125,\n",
       " 0.005462646484375,\n",
       " 0.3203125,\n",
       " -0.314453125,\n",
       " -0.37890625,\n",
       " 0.06689453125,\n",
       " -0.166015625,\n",
       " -0.2578125,\n",
       " 0.1728515625,\n",
       " 0.25390625,\n",
       " -0.240234375,\n",
       " -0.185546875,\n",
       " -0.318359375,\n",
       " -0.034912109375,\n",
       " 0.12255859375,\n",
       " -0.00128173828125,\n",
       " -0.310546875,\n",
       " -0.419921875,\n",
       " 0.1640625,\n",
       " 0.474609375,\n",
       " -0.059814453125,\n",
       " -0.44140625,\n",
       " -0.1337890625,\n",
       " -0.380859375,\n",
       " -0.0498046875,\n",
       " 0.376953125,\n",
       " 0.1787109375,\n",
       " 0.216796875,\n",
       " -0.158203125,\n",
       " -0.130859375,\n",
       " -0.10546875,\n",
       " 0.0830078125,\n",
       " -0.2431640625,\n",
       " -0.2734375,\n",
       " -0.0033111572265625,\n",
       " 0.2138671875,\n",
       " -0.2412109375,\n",
       " -0.061279296875,\n",
       " 0.06884765625,\n",
       " -0.3203125,\n",
       " 0.275390625,\n",
       " -0.142578125,\n",
       " -0.23828125,\n",
       " -0.07177734375,\n",
       " -0.09814453125,\n",
       " -0.1923828125,\n",
       " -0.318359375,\n",
       " -0.07421875,\n",
       " 0.0169677734375,\n",
       " -0.10546875,\n",
       " -0.458984375,\n",
       " -0.423828125,\n",
       " -0.4921875,\n",
       " 0.11669921875,\n",
       " 0.33984375,\n",
       " -0.054443359375,\n",
       " 0.330078125,\n",
       " -0.20703125,\n",
       " 0.01611328125,\n",
       " 0.0084228515625,\n",
       " 0.2314453125,\n",
       " 0.005615234375,\n",
       " 0.40625,\n",
       " -0.361328125,\n",
       " 0.046875,\n",
       " -0.28125,\n",
       " 0.29296875,\n",
       " -0.25390625,\n",
       " 0.19921875,\n",
       " 0.25,\n",
       " -0.275390625,\n",
       " 0.2001953125,\n",
       " -0.19140625,\n",
       " -0.373046875,\n",
       " 0.416015625,\n",
       " -0.14453125,\n",
       " -0.4140625,\n",
       " 0.109375,\n",
       " -0.1611328125,\n",
       " -0.032958984375,\n",
       " 0.298828125,\n",
       " 0.052001953125,\n",
       " 0.333984375,\n",
       " -0.2578125,\n",
       " 0.203125,\n",
       " 0.21484375,\n",
       " 0.07861328125,\n",
       " 0.1962890625,\n",
       " -0.10546875,\n",
       " 0.6640625,\n",
       " 0.12353515625,\n",
       " 0.09814453125,\n",
       " 0.076171875,\n",
       " 0.1806640625,\n",
       " 0.3671875,\n",
       " 0.205078125,\n",
       " 0.28515625,\n",
       " 0.275390625,\n",
       " -0.1630859375,\n",
       " 0.005859375,\n",
       " -0.408203125,\n",
       " 0.462890625,\n",
       " -0.49609375,\n",
       " 0.08203125,\n",
       " -0.2021484375,\n",
       " 0.189453125,\n",
       " 0.40234375,\n",
       " -0.2490234375,\n",
       " -0.251953125,\n",
       " 0.2099609375,\n",
       " 0.2275390625,\n",
       " -0.26953125,\n",
       " 0.265625,\n",
       " -0.1259765625,\n",
       " -0.0062255859375,\n",
       " -0.2275390625,\n",
       " -0.1484375,\n",
       " 0.31640625,\n",
       " 0.01300048828125,\n",
       " -0.1279296875,\n",
       " 0.302734375,\n",
       " 0.330078125,\n",
       " 0.60546875,\n",
       " 0.2470703125,\n",
       " -0.1630859375,\n",
       " -0.06787109375,\n",
       " 0.1669921875,\n",
       " -0.2109375,\n",
       " 0.0216064453125,\n",
       " -0.0223388671875,\n",
       " -0.10400390625,\n",
       " 0.095703125,\n",
       " 0.26953125,\n",
       " 0.07080078125,\n",
       " -0.02587890625,\n",
       " -0.09521484375,\n",
       " 0.1337890625,\n",
       " -0.024169921875,\n",
       " -0.3671875,\n",
       " -0.0791015625,\n",
       " 0.37109375,\n",
       " -0.053955078125,\n",
       " -0.400390625,\n",
       " 0.53125,\n",
       " 0.07763671875,\n",
       " 0.357421875,\n",
       " 0.031494140625,\n",
       " 0.002593994140625,\n",
       " 0.0235595703125,\n",
       " -0.150390625,\n",
       " -0.019775390625,\n",
       " -0.06591796875,\n",
       " -0.16796875,\n",
       " 0.1298828125,\n",
       " -0.318359375,\n",
       " -0.4140625,\n",
       " -0.11962890625,\n",
       " 0.2138671875,\n",
       " -0.4765625,\n",
       " -0.34375,\n",
       " 0.08935546875,\n",
       " 0.041015625,\n",
       " -0.236328125,\n",
       " -0.1865234375,\n",
       " -0.060546875,\n",
       " -0.1767578125,\n",
       " -0.09228515625,\n",
       " -0.2041015625,\n",
       " 0.408203125,\n",
       " 0.08984375,\n",
       " 0.0108642578125,\n",
       " 0.11962890625,\n",
       " 0.044921875,\n",
       " -0.3515625,\n",
       " -0.0634765625,\n",
       " -0.373046875,\n",
       " -0.126953125,\n",
       " 0.00445556640625,\n",
       " 0.5390625,\n",
       " -0.20703125,\n",
       " -0.01904296875,\n",
       " 0.58984375,\n",
       " 0.255859375,\n",
       " 0.279296875,\n",
       " 0.328125,\n",
       " -0.43359375,\n",
       " 0.2021484375,\n",
       " -0.1337890625,\n",
       " 0.1875,\n",
       " -0.306640625,\n",
       " -0.251953125,\n",
       " -0.29296875,\n",
       " -0.002166748046875,\n",
       " 0.017333984375,\n",
       " 0.06982421875,\n",
       " -0.01055908203125,\n",
       " 0.337890625,\n",
       " 0.1435546875,\n",
       " 0.111328125,\n",
       " 0.0732421875,\n",
       " 0.00799560546875,\n",
       " 0.0252685546875,\n",
       " 0.1767578125,\n",
       " -0.0140380859375,\n",
       " 0.061767578125,\n",
       " 0.251953125,\n",
       " -0.2421875,\n",
       " 0.2197265625,\n",
       " -0.01953125,\n",
       " -0.380859375,\n",
       " 0.244140625,\n",
       " -0.251953125,\n",
       " 0.1806640625,\n",
       " -0.357421875,\n",
       " 0.044189453125,\n",
       " -0.1455078125,\n",
       " 0.353515625,\n",
       " 0.056396484375,\n",
       " -0.0081787109375,\n",
       " 0.21484375,\n",
       " -0.1611328125,\n",
       " -0.0206298828125,\n",
       " 0.1552734375,\n",
       " 0.2119140625,\n",
       " 0.30859375,\n",
       " -0.01361083984375,\n",
       " -0.05126953125,\n",
       " 0.024169921875,\n",
       " 0.1962890625,\n",
       " -0.27734375,\n",
       " -0.1337890625,\n",
       " 0.07373046875,\n",
       " 0.42578125,\n",
       " 0.177734375,\n",
       " -0.333984375,\n",
       " -0.314453125,\n",
       " -0.2734375,\n",
       " -0.458984375,\n",
       " 0.29296875,\n",
       " -0.193359375,\n",
       " 0.2021484375,\n",
       " -0.5234375,\n",
       " -0.296875,\n",
       " 0.08984375,\n",
       " 0.150390625,\n",
       " 0.1689453125,\n",
       " -0.2470703125,\n",
       " -0.306640625,\n",
       " 0.09814453125,\n",
       " 0.1591796875,\n",
       " -0.384765625,\n",
       " -0.059814453125,\n",
       " 0.208984375,\n",
       " -0.2060546875,\n",
       " 0.0703125,\n",
       " -0.265625,\n",
       " -0.1640625,\n",
       " -0.197265625,\n",
       " -0.37109375,\n",
       " -0.09814453125,\n",
       " -0.275390625,\n",
       " -0.251953125,\n",
       " -0.00244140625,\n",
       " 0.279296875,\n",
       " -0.275390625,\n",
       " 0.392578125,\n",
       " -0.0390625,\n",
       " 0.376953125,\n",
       " 0.15625,\n",
       " -0.255859375,\n",
       " -0.1337890625,\n",
       " -0.326171875,\n",
       " 0.23828125,\n",
       " -0.220703125,\n",
       " -0.412109375,\n",
       " -0.3671875,\n",
       " -0.1884765625,\n",
       " 0.2099609375,\n",
       " 0.51953125,\n",
       " 0.326171875,\n",
       " -0.482421875,\n",
       " 0.298828125,\n",
       " 0.02978515625,\n",
       " -0.33203125,\n",
       " 0.48046875,\n",
       " 0.27734375,\n",
       " -0.10791015625,\n",
       " -0.2451171875,\n",
       " -0.031982421875,\n",
       " 0.361328125,\n",
       " -0.216796875,\n",
       " 0.051025390625,\n",
       " 0.21484375,\n",
       " -0.36328125,\n",
       " -0.1201171875,\n",
       " 0.21484375,\n",
       " 0.48046875,\n",
       " 0.060302734375,\n",
       " 0.435546875,\n",
       " 0.36328125,\n",
       " -0.060302734375,\n",
       " 0.064453125,\n",
       " -0.039794921875,\n",
       " -0.0849609375,\n",
       " -0.255859375,\n",
       " 0.46484375,\n",
       " -0.048095703125,\n",
       " -0.30859375,\n",
       " 0.1650390625,\n",
       " -0.0179443359375,\n",
       " 0.384765625,\n",
       " -0.333984375,\n",
       " -0.71484375,\n",
       " -0.302734375,\n",
       " 0.283203125,\n",
       " -0.298828125,\n",
       " 0.3828125,\n",
       " -0.302734375,\n",
       " -0.53515625,\n",
       " -0.51171875,\n",
       " -0.06787109375,\n",
       " -0.1259765625,\n",
       " -0.1904296875,\n",
       " -0.08984375,\n",
       " 0.37109375,\n",
       " -0.115234375,\n",
       " 0.11328125,\n",
       " 0.365234375,\n",
       " 0.40234375,\n",
       " 0.19140625,\n",
       " 0.1962890625,\n",
       " -0.051025390625,\n",
       " -0.1455078125,\n",
       " -0.00110626220703125,\n",
       " 0.134765625,\n",
       " 0.00537109375,\n",
       " -0.06201171875,\n",
       " 0.1494140625,\n",
       " -0.031494140625,\n",
       " -0.011474609375,\n",
       " 0.078125,\n",
       " 0.06591796875,\n",
       " 0.208984375,\n",
       " 0.0703125,\n",
       " -0.03173828125,\n",
       " 0.11328125,\n",
       " -0.2216796875,\n",
       " 0.036865234375,\n",
       " -0.1630859375,\n",
       " -0.2119140625,\n",
       " -0.0419921875,\n",
       " 0.279296875,\n",
       " 0.1640625,\n",
       " -0.26953125,\n",
       " -0.232421875,\n",
       " 0.240234375,\n",
       " -0.263671875,\n",
       " 0.0294189453125,\n",
       " 0.1171875,\n",
       " 0.310546875,\n",
       " 0.33984375,\n",
       " 0.30078125,\n",
       " 0.111328125,\n",
       " -0.1943359375,\n",
       " -0.08349609375,\n",
       " 0.068359375,\n",
       " -0.365234375,\n",
       " -0.330078125,\n",
       " -0.326171875,\n",
       " -0.022216796875,\n",
       " -0.060546875,\n",
       " 0.07421875,\n",
       " -0.2431640625,\n",
       " -0.1328125,\n",
       " -0.14453125,\n",
       " 0.39453125,\n",
       " 0.162109375,\n",
       " 0.0380859375,\n",
       " 0.357421875,\n",
       " -0.3359375,\n",
       " -0.208984375,\n",
       " 0.039306640625,\n",
       " -0.298828125,\n",
       " -0.10595703125,\n",
       " 0.0693359375,\n",
       " 0.04833984375,\n",
       " 0.02734375,\n",
       " 0.2333984375,\n",
       " 0.0986328125,\n",
       " -0.33203125,\n",
       " -0.1513671875,\n",
       " 0.240234375,\n",
       " -0.051025390625,\n",
       " -0.08544921875,\n",
       " 0.0274658203125,\n",
       " -0.353515625,\n",
       " 0.490234375,\n",
       " 0.2001953125,\n",
       " -0.162109375,\n",
       " -0.07568359375,\n",
       " 0.169921875,\n",
       " -0.279296875,\n",
       " 0.1533203125,\n",
       " -0.4765625,\n",
       " 0.056884765625,\n",
       " -0.1630859375,\n",
       " 0.267578125,\n",
       " -0.1376953125,\n",
       " 0.00677490234375,\n",
       " -0.00225830078125,\n",
       " -0.39453125,\n",
       " 0.271484375,\n",
       " -0.142578125,\n",
       " -0.039306640625,\n",
       " 0.052734375,\n",
       " -0.2265625,\n",
       " -0.1923828125,\n",
       " -0.048583984375,\n",
       " -0.30078125,\n",
       " 0.03515625,\n",
       " 0.1123046875,\n",
       " 0.060546875,\n",
       " -0.15234375,\n",
       " 0.3828125,\n",
       " -0.0546875,\n",
       " 0.09716796875,\n",
       " -0.166015625,\n",
       " -0.291015625,\n",
       " -0.43359375,\n",
       " 0.2255859375,\n",
       " -0.2734375,\n",
       " -0.29296875,\n",
       " -0.302734375,\n",
       " 0.07080078125,\n",
       " -0.330078125,\n",
       " -0.1328125,\n",
       " 0.291015625,\n",
       " 0.291015625,\n",
       " -0.1630859375,\n",
       " -0.0791015625,\n",
       " 0.0693359375,\n",
       " -0.0947265625,\n",
       " 0.2001953125,\n",
       " -0.0595703125,\n",
       " -0.328125,\n",
       " -0.11474609375,\n",
       " -0.21484375,\n",
       " -0.09619140625,\n",
       " 0.10302734375,\n",
       " 0.404296875,\n",
       " 0.2109375,\n",
       " 0.11328125,\n",
       " -0.0966796875,\n",
       " 0.205078125,\n",
       " -0.044677734375,\n",
       " -0.17578125,\n",
       " 0.169921875,\n",
       " 0.1220703125,\n",
       " 0.2138671875,\n",
       " -0.193359375,\n",
       " 0.0478515625,\n",
       " -0.3125,\n",
       " -0.2333984375,\n",
       " 0.10791015625,\n",
       " -0.5,\n",
       " -0.34375,\n",
       " 0.380859375,\n",
       " 0.259765625,\n",
       " -0.447265625,\n",
       " -0.1669921875,\n",
       " -0.376953125,\n",
       " -0.10498046875,\n",
       " -0.04833984375,\n",
       " -0.050537109375,\n",
       " -0.034423828125,\n",
       " -0.007415771484375,\n",
       " -0.228515625,\n",
       " -0.37109375,\n",
       " -0.21484375,\n",
       " -0.1435546875,\n",
       " -0.265625,\n",
       " -0.024658203125,\n",
       " -0.203125,\n",
       " -0.404296875,\n",
       " 0.05126953125,\n",
       " 0.314453125,\n",
       " -0.1357421875,\n",
       " 0.28125,\n",
       " -0.0224609375,\n",
       " -0.326171875,\n",
       " 0.1083984375,\n",
       " -0.0322265625,\n",
       " -0.0703125,\n",
       " 0.06591796875,\n",
       " -0.26953125,\n",
       " -0.4453125,\n",
       " -0.2421875,\n",
       " 0.2333984375,\n",
       " -0.10986328125,\n",
       " 0.2490234375,\n",
       " 0.251953125,\n",
       " -0.486328125,\n",
       " 0.1123046875,\n",
       " -0.006011962890625,\n",
       " -0.2421875,\n",
       " 0.10888671875,\n",
       " -0.486328125,\n",
       " -0.2314453125,\n",
       " 0.12451171875,\n",
       " -0.080078125,\n",
       " -0.0244140625,\n",
       " 0.13671875,\n",
       " -0.1328125,\n",
       " -0.36328125,\n",
       " -0.083984375,\n",
       " -0.34765625,\n",
       " -0.146484375,\n",
       " -0.08056640625,\n",
       " -0.0849609375,\n",
       " 0.1875,\n",
       " -0.44140625,\n",
       " -0.65234375,\n",
       " 0.31640625,\n",
       " -0.2470703125,\n",
       " -0.11328125,\n",
       " 0.2294921875,\n",
       " 0.076171875,\n",
       " 0.28125,\n",
       " -0.2470703125,\n",
       " 0.0810546875,\n",
       " 0.119140625,\n",
       " 0.1259765625,\n",
       " -0.2294921875,\n",
       " 0.01336669921875,\n",
       " -0.28125,\n",
       " -0.035400390625,\n",
       " 0.2060546875,\n",
       " 0.0299072265625]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(text,\n",
    "          padding='max_length',\n",
    "          max_length=4096,\n",
    "          truncation=True,\n",
    "          return_tensors='pt').to(device)\n",
    "outputs = longformer(**input_ids)\n",
    "outputs.pooler_output.squeeze().detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"Main Category: Books\\nTitle: Chaucer\\nSubtitle: Hardcover – Import, January 1, 2004\\nAuthor: {'avatar': 'https://m.media-amazon.com/images/I/21Je2zja9pL._SY600_.jpg', 'name': 'Peter Ackroyd', 'about': ['Peter Ackroyd, (born 5 October 1949) is an English biographer, novelist and critic with a particular interest in the history and culture of London. For his novels about English history and culture and his biographies of, among others, William Blake, Charles Dickens, T. S. Eliot and Sir Thomas More, he won the Somerset Maugham Award and two Whitbread Awards. He is noted for the volume of work he has produced, the range of styles therein, his skill at assuming different voices and the depth of his research.', 'He was elected a fellow of the Royal Society of Literature in 1984 and appointed a Commander of the Order of the British Empire in 2003.', 'Bio from Wikipedia, the free encyclopedia.']}\\nAverage Rating: 4.5\\nFeatures: []\\nDescription: []\\nCategories: ['Books', 'Literature & Fiction', 'History & Criticism']\\n\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"Main Category: {main_category}\n",
    "Title: {title}\n",
    "Subtitle: {subtitle}\n",
    "Author: {author}\n",
    "Average Rating: {average_rating}\n",
    "Features: {features}\n",
    "Description: {description}\n",
    "Categories: {categories}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.invoke(input=meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
