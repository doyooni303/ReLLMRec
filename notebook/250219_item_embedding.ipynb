{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"/home/doyooni303/experiments/LLMRec/ReLLMRec\")\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from src.utils import open_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder = \"/home/doyooni303/experiments/LLMRec/data/amazon/Books\"\n",
    "fname = \"Books\"\n",
    "itemmap = json.load(open(os.path.join(folder, f\"{fname}_itemmap.json\"),\"r\"))\n",
    "meta_name_dict = json.load(open(os.path.join(folder, f\"{fname}_meta_name_dict.json\"),\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): LongformerPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 30371.50it/s]\n"
     ]
    }
   ],
   "source": [
    "iidx_list, text_list = [], [],\n",
    "meta_keys = list(meta_name_dict.keys())\n",
    "for i,(iid, idx) in tqdm(enumerate(itemmap.items())):\n",
    "    item_text = \", \".join([f\"{key.upper()}: {meta_name_dict[key][str(idx)]}\" for key in meta_keys])\n",
    "    iidx_list.append(idx)\n",
    "    text_list.append(item_text)\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 970/970 [03:37<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in tqdm(range(0, len(text_list), 1000)):\n",
    "    tokenized = tokenizer(text_list[i:i+1000], padding=False, truncation=False)\n",
    "    lengths.extend([len(token) for token in tokenized['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.percentile(lengths, q=99.7) # 약 99.7%의 text는 1925 개 이하임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(303)\n",
    "\n",
    "dim = 768\n",
    "embeddings = nn.ParameterList([[nn.Parameter(torch.randn(dim))]])\n",
    "\n",
    "batch_size = 16\n",
    "max_length = 2048\n",
    "\n",
    "for i in tqdm(range(0, len(iidx_list), batch_size)):\n",
    "    batch_idxs = [int(idx) for idx in iidx_list[i:i+batch_size]]\n",
    "    batch_texts = text_list[i:i+batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    outputs = model(**inputs.to(device)).pooler_output.cpu().detach()\n",
    "    # embeddings.weight.data[batch_idxs] = outputs\n",
    "    embeddings.append(nn.Parameter(outputs))\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_list, idx_list, tokenizer, max_length):\n",
    "        self.texts = text_list\n",
    "        self.idxs = idx_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        text = self.texts[i]\n",
    "        idx = self.idxs[i]\n",
    "        \n",
    "        # Pre-tokenize the text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Squeeze to remove batch dimension since DataLoader will add it\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'idx': torch.tensor(idx)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    idxs = torch.stack([item['idx'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'idxs': idxs\n",
    "    }\n",
    "\n",
    "# Set random seed\n",
    "torch.random.manual_seed(303)\n",
    "\n",
    "# Initialize storage for embeddings\n",
    "dim = 768\n",
    "all_embeddings = torch.zeros((len(text_list)+1, dim))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(text_list, iidx_list, tokenizer, max_length=2048)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,  # Increased batch size\n",
    "    shuffle=False,\n",
    "    num_workers=2,  # Parallel data loading\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Process batches with gradient disabled\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Move batch to GPU\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs).pooler_output\n",
    "        \n",
    "        # Store embeddings\n",
    "        all_embeddings[batch['idxs']] = outputs.cpu()\n",
    "\n",
    "# Convert to Parameter if needed\n",
    "embeddings = nn.Parameter(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of 1\n",
      "Pre-tokenizing all texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 8/8 [00:16<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import gc\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_list, idx_list, tokenizer):\n",
    "        print(\"Pre-tokenizing all texts...\")\n",
    "        self.encodings = tokenizer(\n",
    "            text_list,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1925,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.idxs = torch.tensor(idx_list, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][i],\n",
    "            'attention_mask': self.encodings['attention_mask'][i],\n",
    "            'idx': self.idxs[i]\n",
    "        }\n",
    "\n",
    "def process_large_dataset(text_list, iidx_list, model, tokenizer, device, \n",
    "                         batch_size=128, chunk_size=50000):\n",
    "    dim = 768\n",
    "    total_size = len(text_list)\n",
    "    # Initialize as float16 to match autocast dtype\n",
    "    all_embeddings = torch.zeros((total_size, dim), dtype=torch.float16)\n",
    "    \n",
    "    for chunk_start in range(0, total_size, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_size)\n",
    "        \n",
    "        print(f\"Processing chunk {chunk_start//chunk_size + 1} of {(total_size-1)//chunk_size + 1}\")\n",
    "        \n",
    "        chunk_dataset = TextDataset(\n",
    "            text_list[chunk_start:chunk_end],\n",
    "            iidx_list[chunk_start:chunk_end],\n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            chunk_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad(), autocast():\n",
    "            for batch in tqdm(dataloader):\n",
    "                inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(device, non_blocking=True),\n",
    "                    'attention_mask': batch['attention_mask'].to(device, non_blocking=True)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**inputs).pooler_output\n",
    "                # Store directly in half precision\n",
    "                all_embeddings[batch['idx']] = outputs.cpu()\n",
    "        \n",
    "        del chunk_dataset, dataloader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert back to float32 at the end if needed\n",
    "    all_embeddings = all_embeddings.float()\n",
    "    return nn.Parameter(all_embeddings)\n",
    "\n",
    "# Main execution\n",
    "torch.random.manual_seed(303)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Process the texts\n",
    "embeddings = process_large_dataset(\n",
    "    text_list=text_list,\n",
    "    iidx_list=iidx_list,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    batch_size=128,\n",
    "    chunk_size=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, torch.Size([969146, 768]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "emb = torch.load(\"/home/doyooni303/experiments/LLMRec/data/amazon/Books/Books_item_embeddings.pt\")\n",
    "type(emb), emb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
