{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "# from pathlib import Path\n",
    "# Add the project root directory to Python path\n",
    "# project_root = str(Path(__file__).parent.parent.parent)\n",
    "\n",
    "sys.path.append(\"/home/doyooni303/experiments/sequential_recsys/ReLLMRec\")\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from src.utils import open_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/data3/doyoon/sequential_recsys/datasets/\"\n",
    "dataset = \"amazon\"\n",
    "category = \"Books\"\n",
    "history = json.load(open(os.path.join(dataset_path, dataset, category,f\"{category}_history.json\"),'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_dict = json.load(open(os.path.join(dataset_path, dataset, category,f\"{category}_itemidx2meta.json\"),'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4448181,\n",
       " {'main_category': 'Books',\n",
       "  'title': 'Chaucer',\n",
       "  'subtitle': 'Hardcover – Import, January 1, 2004',\n",
       "  'author': {'avatar': 'https://m.media-amazon.com/images/I/21Je2zja9pL._SY600_.jpg',\n",
       "   'name': 'Peter Ackroyd',\n",
       "   'about': ['Peter Ackroyd, (born 5 October 1949) is an English biographer, novelist and critic with a particular interest in the history and culture of London. For his novels about English history and culture and his biographies of, among others, William Blake, Charles Dickens, T. S. Eliot and Sir Thomas More, he won the Somerset Maugham Award and two Whitbread Awards. He is noted for the volume of work he has produced, the range of styles therein, his skill at assuming different voices and the depth of his research.',\n",
       "    'He was elected a fellow of the Royal Society of Literature in 1984 and appointed a Commander of the Order of the British Empire in 2003.',\n",
       "    'Bio from Wikipedia, the free encyclopedia.']},\n",
       "  'average_rating': 4.5,\n",
       "  'rating_number': 29,\n",
       "  'features': [],\n",
       "  'description': [],\n",
       "  'price': 8.23,\n",
       "  'images': [{'large': 'https://m.media-amazon.com/images/I/41X61VPJYKL._SX334_BO1,204,203,200_.jpg',\n",
       "    'variant': 'MAIN'}],\n",
       "  'videos': [],\n",
       "  'store': 'Peter Ackroyd (Author)',\n",
       "  'categories': ['Books', 'Literature & Fiction', 'History & Criticism'],\n",
       "  'details': {'Publisher': 'Chatto & Windus; First Edition (January 1, 2004)',\n",
       "   'Language': 'English',\n",
       "   'Hardcover': '196 pages',\n",
       "   'ISBN 10': '0701169850',\n",
       "   'ISBN 13': '978-0701169855',\n",
       "   'Item Weight': '10.1 ounces',\n",
       "   'Dimensions': '5.39 x 0.71 x 7.48 inches'},\n",
       "  'parent_asin': '0701169850',\n",
       "  'bought_together': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items_dict), items_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function cosine_similarity in module torch:\n",
      "\n",
      "cosine_similarity(...)\n",
      "    cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n",
      "    \n",
      "    Returns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\n",
      "    to a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\n",
      "    squeezed (see :func:`torch.squeeze`), resulting in the\n",
      "    output tensor having 1 fewer dimension.\n",
      "    \n",
      "    .. math ::\n",
      "        \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2, \\epsilon) \\cdot \\max(\\Vert x_2 \\Vert _2, \\epsilon)}\n",
      "    \n",
      "    Supports :ref:`type promotion <type-promotion-doc>`.\n",
      "    \n",
      "    Args:\n",
      "        x1 (Tensor): First input.\n",
      "        x2 (Tensor): Second input.\n",
      "        dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n",
      "        eps (float, optional): Small value to avoid division by zero.\n",
      "            Default: 1e-8\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> input1 = torch.randn(100, 128)\n",
      "        >>> input2 = torch.randn(100, 128)\n",
      "        >>> output = F.cosine_similarity(input1, input2)\n",
      "        >>> print(output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:00<00:00, 848.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing took 0.49 seconds\n",
      "Top 10 similarity scores for first query: tensor([0.1834, 0.1688, 0.1677, 0.1676, 0.1651, 0.1629, 0.1624, 0.1622, 0.1615,\n",
      "        0.1613], device='cuda:0')\n",
      "Indices of top 10 matches for first query: tensor([  98867,  989619,  517361, 3494341,  572961, 2909426, 3011890, 3786693,\n",
      "        1731871, 4004155], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_top_k_similarities(query, candidates, k=10, batch_size=20000):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity and return top-k matches for each query\n",
    "    \n",
    "    Args:\n",
    "        query: tensor of shape (n_queries, embedding_dim)\n",
    "        candidates: tensor of shape (n_candidates, embedding_dim)\n",
    "        k: number of top matches to return\n",
    "        batch_size: size of candidates batches to process at once\n",
    "    \n",
    "    Returns:\n",
    "        top_scores: tensor of shape (n_queries, k) - top similarity scores\n",
    "        top_indices: tensor of shape (n_queries, k) - indices of top matches\n",
    "    \"\"\"\n",
    "    \n",
    "    n_queries = query.shape[0]\n",
    "    n_candidates = candidates.shape[0]\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    query_normalized = torch.nn.functional.normalize(query, p=2, dim=1)\n",
    "    candidates_normalized = torch.nn.functional.normalize(candidates, p=2, dim=1)\n",
    "    \n",
    "    # Initialize tensors to store top-k scores and indices\n",
    "    top_scores = torch.zeros((n_queries, k), device=query.device)\n",
    "    top_indices = torch.zeros((n_queries, k), dtype=torch.long, device=query.device)\n",
    "    \n",
    "    # For each query, maintain a min-heap of top-k scores\n",
    "    current_scores, current_indices = torch.topk(\n",
    "        torch.zeros(n_queries, k, device=query.device),\n",
    "        k=k, \n",
    "        dim=1, \n",
    "        largest=True\n",
    "    )\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, n_candidates, batch_size)):\n",
    "        end_idx = min(i + batch_size, n_candidates)\n",
    "        current_batch_size = end_idx - i\n",
    "        \n",
    "        # Calculate similarity for current batch\n",
    "        batch_similarity = torch.matmul(\n",
    "            query_normalized,\n",
    "            candidates_normalized[i:end_idx].t()\n",
    "        )\n",
    "        \n",
    "        # For each query, update top-k scores and indices\n",
    "        if i == 0:\n",
    "            # For first batch, simply get top-k\n",
    "            top_scores, top_indices = torch.topk(batch_similarity, min(k, current_batch_size), dim=1)\n",
    "            top_indices = top_indices + i  # Add offset\n",
    "        else:\n",
    "            # Concatenate previous top-k with current batch similarities\n",
    "            all_scores = torch.cat([top_scores, batch_similarity], dim=1)\n",
    "            all_indices = torch.cat([\n",
    "                top_indices,\n",
    "                torch.arange(i, end_idx, device=query.device).expand(n_queries, current_batch_size)\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Get top-k from all scores\n",
    "            top_scores, indices = torch.topk(all_scores, k, dim=1)\n",
    "            # Update top indices using the selected indices\n",
    "            top_indices = torch.gather(all_indices, 1, indices)\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print(f\"Processing took {end - start:.2f} seconds\")\n",
    "    \n",
    "    return top_scores, top_indices\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(f'cuda')\n",
    "query = torch.randn(16, 768).to(device)  # Your query tensor\n",
    "candidates = torch.randn(4448181, 768).to(device)  # Your candidates tensor\n",
    "\n",
    "# Get top 10 most similar candidates for each query\n",
    "start = time.perf_counter()\n",
    "top_scores, top_indices = get_top_k_similarities(query, candidates, k=10)\n",
    "\n",
    "# Print results for first query\n",
    "print(\"Top 10 similarity scores for first query:\", top_scores[0])\n",
    "print(\"Indices of top 10 matches for first query:\", top_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, items: Dict, item_indices:List[str], meta_keys:List[str], tokenizer, max_length=4096):\n",
    "        self.items = items\n",
    "        self.item_indices = item_indices\n",
    "        self.meta_keys = meta_keys\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def get_texts(self, item: dict, keys:List[str], category: str = \"Books\"):\n",
    "        def _get_text_Books(item: dict, keys: list):\n",
    "            text = \"\"\n",
    "            for key in keys:\n",
    "                try:\n",
    "                    if isinstance(item[key],str):\n",
    "                        text += f\"{key}: {item[key]}</s>\"\n",
    "                    elif isinstance(item[key],list):\n",
    "                        if len(item[key])>0:\n",
    "                            add = \", \".join(item[key])\n",
    "                            text += f\"{key}: {add}</s>\"\n",
    "                    if key=='author':\n",
    "                        text += f\"author: {item['author']['name']}</s>\"\n",
    "                except:\n",
    "                    text += f\"{key}: None</s>\"\n",
    "            return text\n",
    "        \n",
    "        if category == \"Books\":\n",
    "            text =  _get_text_Books(item, self.meta_keys)\n",
    "        \n",
    "        return text\n",
    "            \n",
    "    def __getitem__(self, idx:int):\n",
    "        item = self.items[str(idx)]\n",
    "        text = self.get_texts(item, self.meta_keys)\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'item_idx': str(idx)\n",
    "        }\n",
    "\n",
    "class EmbeddingStorage(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "    def forward(self, indices):\n",
    "        return self.embedding(indices)\n",
    "    \n",
    "    def update_embeddings(self, indices: torch.Tensor, new_embeddings: torch.Tensor):\n",
    "        \"\"\"Update specific embeddings by indices\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight.data[indices] = new_embeddings\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, \n",
    "                 model_name: str = 'allenai/longformer-base-4096',\n",
    "                 batch_size: int = 8,\n",
    "                 num_embeddings: int = 4448181,\n",
    "                 embedding_dim: int = 768,\n",
    "                 use_ddp: bool = False):  # Add this parameter\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_ddp = use_ddp\n",
    "        \n",
    "        # Initialize distributed environment if using DDP\n",
    "        if self.use_ddp:\n",
    "            if not dist.is_initialized():\n",
    "                setup_distributed()\n",
    "            self.rank = dist.get_rank()\n",
    "            self.world_size = dist.get_world_size()\n",
    "            self.device = torch.device(f'cuda:{self.rank}')\n",
    "        else:\n",
    "            self.rank = 0\n",
    "            self.world_size = 1\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Set device\n",
    "        if self.use_ddp:\n",
    "            torch.cuda.set_device(self.device)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        base_model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model = base_model.to(self.device)\n",
    "        \n",
    "        # Wrap with DDP if using distributed training\n",
    "        if self.use_ddp:\n",
    "            self.model = DDP(self.model, \n",
    "                           device_ids=[self.rank],\n",
    "                           output_device=self.rank)\n",
    "        \n",
    "        # Initialize embedding storage\n",
    "        print(\"Initialize embedding storage...\")\n",
    "        self.embedding_storage = EmbeddingStorage(num_embeddings, embedding_dim)\n",
    "        self.embedding_storage = self.embedding_storage.to(self.device)\n",
    "        if self.use_ddp:\n",
    "            self.embedding_storage = DDP(self.embedding_storage,\n",
    "                                      device_ids=[self.rank],\n",
    "                                      output_device=self.rank)\n",
    "\n",
    "    def set_dataset_and_dataloader(self,\n",
    "                                 items: dict,\n",
    "                                 item_indices: List[str],\n",
    "                                 keys: List[str]):\n",
    "        # Create dataset\n",
    "        self.dataset = BookDataset(items, item_indices, keys, self.tokenizer)\n",
    "        \n",
    "        # Create sampler based on DDP usage\n",
    "        if self.use_ddp:\n",
    "            self.sampler = DistributedSampler(self.dataset, \n",
    "                                            num_replicas=self.world_size,\n",
    "                                            rank=self.rank)\n",
    "        else:\n",
    "            self.sampler = None\n",
    "        \n",
    "        self.dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.sampler,\n",
    "            shuffle=(self.sampler is None),  # Only shuffle if not using DDP\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    def generate_and_store_embeddings(self, \n",
    "                                items: dict,\n",
    "                                item_indices: List[str],\n",
    "                                keys: List[str]):\n",
    "        \"\"\"\n",
    "        Generate embeddings and store them in the embedding layer\n",
    "        \n",
    "        Args:\n",
    "            items: List of item dictionaries containing metadata\n",
    "            item_indices: List of indices corresponding to items\n",
    "            keys: List of keys to extract from metadata\n",
    "        \"\"\"\n",
    "        # Set dataset and dataloader if not already set\n",
    "        if not hasattr(self, 'dataloader'):\n",
    "            self.set_dataset_and_dataloader(items, item_indices, keys)\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.dataloader, \n",
    "                            desc=f\"Generating embeddings (GPU {self.device.index})\"): \n",
    "                # Move batch to GPU\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                if isinstance(batch['item_idx'],List):\n",
    "                    item_indices_batch = torch.tensor([int(idx) for idx in batch['item_idx']]).to(self.device, dtype=torch.int)\n",
    "                                        \n",
    "                # Get model outputs\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                \n",
    "                # Get pooler output\n",
    "                batch_embeddings = outputs.pooler_output\n",
    "                \n",
    "                # Update embedding storage\n",
    "                # For single GPU, no need to access .module\n",
    "                if hasattr(self.embedding_storage, 'module'):\n",
    "                    self.embedding_storage.module.update_embeddings(\n",
    "                        item_indices_batch,\n",
    "                        batch_embeddings\n",
    "                    )\n",
    "                else:\n",
    "                    self.embedding_storage.update_embeddings(\n",
    "                        item_indices_batch,\n",
    "                        batch_embeddings\n",
    "                    )\n",
    "\n",
    "        # Only call barrier if using DDP\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'run_ddp' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m rank \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     46\u001b[0m world_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 48\u001b[0m mp\u001b[39m.\u001b[39;49mspawn(\n\u001b[1;32m     49\u001b[0m     run_ddp,\n\u001b[1;32m     50\u001b[0m     args\u001b[39m=\u001b[39;49m(world_size, model_name, items_dict, item_indices, meta_keys),\n\u001b[1;32m     51\u001b[0m     nprocs\u001b[39m=\u001b[39;49mworld_size\n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:282\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    276\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m{\u001b[39;00mstart_method\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:229\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    223\u001b[0m os\u001b[39m.\u001b[39munlink(tf\u001b[39m.\u001b[39mname)\n\u001b[1;32m    224\u001b[0m process \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mProcess(\n\u001b[1;32m    225\u001b[0m     target\u001b[39m=\u001b[39m_wrap,\n\u001b[1;32m    226\u001b[0m     args\u001b[39m=\u001b[39m(fn, i, args, tf\u001b[39m.\u001b[39mname),\n\u001b[1;32m    227\u001b[0m     daemon\u001b[39m=\u001b[39mdaemon,\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    230\u001b[0m error_files\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39mname)\n\u001b[1;32m    231\u001b[0m processes\u001b[39m.\u001b[39mappend(process)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,2\"  # Set the GPU 2 to use\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "def setup_distributed(backend=\"nccl\",\n",
    "                      world_size: int = 2,\n",
    "                      rank: int = 0):\n",
    "    \"\"\"Initialize distributed training environment\"\"\"\n",
    "    print(\"Initialize distributed training environment\")\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(backend=backend,\n",
    "                                world_size=world_size,\n",
    "                                rank=rank,)\n",
    "    \n",
    "def cleanup_distributed():\n",
    "    \"\"\"Clean up distributed training environment\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "\n",
    "\n",
    "# Define run function\n",
    "def run_ddp(rank, world_size, model_name, items, item_indices, keys):\n",
    "    # Note: rank is the first parameter spawned processes receive\n",
    "    setup_distributed(world_size=world_size, rank=rank)\n",
    "    generator = EmbeddingGenerator(model_name=model_name, use_ddp=True)\n",
    "    generator.set_dataset_and_dataloader(items, item_indices, keys)\n",
    "    generator.generate_and_store_embeddings(items, item_indices, keys)\n",
    "    cleanup_distributed()\n",
    "\n",
    "# if __name__ == \"__main__\":  # Add this for notebook\n",
    "\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "item_indices = list(items_dict.keys())\n",
    "meta_keys = ['main_category',\n",
    "'title',\n",
    "'subtitle',\n",
    "'author',\n",
    "'average_rating',\n",
    "'features',\n",
    "'description',\n",
    "'categories',]\n",
    "num_embeddings = len(item_indices)\n",
    "rank = 0\n",
    "world_size = 2\n",
    "\n",
    "mp.spawn(\n",
    "    run_ddp,\n",
    "    args=(world_size, model_name, items_dict, item_indices, meta_keys),\n",
    "    nprocs=world_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For single GPU:\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "item_indices = list(items_dict.keys())\n",
    "meta_keys = ['main_category',\n",
    " 'title',\n",
    " 'subtitle',\n",
    " 'author',\n",
    " 'average_rating',\n",
    " 'features',\n",
    " 'description',\n",
    " 'categories',]\n",
    "num_embeddings = len(item_indices)\n",
    "\n",
    "use_ddp = True\n",
    "\n",
    "generator = EmbeddingGenerator(model_name = model_name, use_ddp=use_ddp)\n",
    "# generator.set_dataset_and_dataloader(items_dict, item_indices, meta_keys)\n",
    "generator.generate_and_store_embeddings(items_dict, item_indices, meta_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def setup_distributed(backend=\"nccl\",\n",
    "                      world_size: int = 2,\n",
    "                      rank: int = 0):\n",
    "    \"\"\"Initialize distributed training environment\"\"\"\n",
    "    print(\"Initialize distributed training environment\")\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(backend=backend,\n",
    "                                world_size=world_size,\n",
    "                                rank=rank,)\n",
    "    \n",
    "def cleanup_distributed():\n",
    "    \"\"\"Clean up distributed training environment\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, items: Dict, item_indices:List[str], meta_keys:List[str], tokenizer_name: str, max_length=4096):\n",
    "        self.items = items\n",
    "        self.item_indices = item_indices\n",
    "        self.meta_keys = meta_keys\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def get_texts(self, item: dict, category: str = \"Books\"):\n",
    "        def _get_text_Books(item: dict, keys: list):\n",
    "            text = \"\"\n",
    "            for key in keys:\n",
    "                try:\n",
    "                    if isinstance(item[key],str):\n",
    "                        text += f\"{key}: {item[key]}</s>\"\n",
    "                    elif isinstance(item[key],list):\n",
    "                        if len(item[key])>0:\n",
    "                            add = \", \".join(item[key])\n",
    "                            text += f\"{key}: {add}</s>\"\n",
    "                    if key=='author':\n",
    "                        text += f\"author: {item['author']['name']}</s>\"\n",
    "                except:\n",
    "                    text += f\"{key}: None</s>\"\n",
    "            return text\n",
    "        \n",
    "        if category == \"Books\":\n",
    "            text =  _get_text_Books(item, self.meta_keys)\n",
    "        \n",
    "        return text\n",
    "            \n",
    "    def __getitem__(self, idx:int):\n",
    "        item = self.items[str(idx)]\n",
    "        text = self._get_text_Books(item, self.meta_keys)\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'item_idx': str(idx)\n",
    "        }\n",
    "\n",
    "class EmbeddingStorage(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "    def forward(self, indices):\n",
    "        return self.embedding(indices)\n",
    "    \n",
    "    def update_embeddings(self, indices: torch.Tensor, new_embeddings: torch.Tensor):\n",
    "        \"\"\"Update specific embeddings by indices\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight.data[indices] = new_embeddings\n",
    "\n",
    "class DistributedEmbeddingGenerator:\n",
    "    def __init__(self, \n",
    "                 model_name: str = 'allenai/longformer-base-4096',\n",
    "                 batch_size: int = 8,\n",
    "                 num_embeddings: int = 4448181,\n",
    "                 embedding_dim: int = 768):\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize distributed environment if not already initialized\n",
    "        if not dist.is_initialized():\n",
    "            print(\"Initialize distributed environment if not already initialized\")\n",
    "            setup_distributed()\n",
    "            \n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(f'cuda:{self.rank}')\n",
    "        torch.cuda.set_device(self.device)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        base_model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Move model to GPU\n",
    "        self.model = base_model.to(self.device)\n",
    "        \n",
    "        # Wrap with DDP\n",
    "        self.model = DDP(self.model, \n",
    "                        device_ids=[self.rank],\n",
    "                        output_device=self.rank)\n",
    "        \n",
    "        # Initialize embedding storage\n",
    "        print(\"Initialize embedding storage...\")\n",
    "        self.embedding_storage = EmbeddingStorage(num_embeddings, embedding_dim)\n",
    "        self.embedding_storage = self.embedding_storage.to(self.device)\n",
    "        self.embedding_storage = DDP(self.embedding_storage,\n",
    "                                   device_ids=[self.rank],\n",
    "                                   output_device=self.rank)\n",
    "\n",
    "    def set_dataset_and_dataloader(self,\n",
    "                                   items: dict,\n",
    "                                   item_indices: List[str],\n",
    "                                   keys: List[str]\n",
    "                                   ):\n",
    "        # Create dataset and distributed sampler\n",
    "        self.dataset = BookDataset(items, item_indices, keys, self.tokenizer)\n",
    "        self.sampler = DistributedSampler(dataset, \n",
    "                                   num_replicas=self.world_size,\n",
    "                                   rank=self.rank)\n",
    "        \n",
    "        self.dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "    def generate_and_store_embeddings(self, \n",
    "                                    items: dict,\n",
    "                                    item_indices: List[str],\n",
    "                                    keys: List[str]):\n",
    "        \"\"\"\n",
    "        Generate embeddings and store them in the embedding layer\n",
    "        \n",
    "        Args:\n",
    "            items: List of item dictionaries containing metadata\n",
    "            item_indices: List of indices corresponding to items\n",
    "            keys: List of keys to extract from metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.dataloader, \n",
    "                            desc=f\"Generating embeddings (GPU {self.rank})\",\n",
    "                            disable=self.rank != 0):\n",
    "                # Move batch to GPU\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                item_indices_batch = batch['item_idx'].to(self.device, dtype=torch.int)\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                \n",
    "                # Get pooler output\n",
    "                batch_embeddings = outputs.pooler_output\n",
    "                \n",
    "                # Update embedding storage\n",
    "                self.embedding_storage.module.update_embeddings(\n",
    "                    item_indices_batch,\n",
    "                    batch_embeddings\n",
    "                )\n",
    "        \n",
    "        # Synchronize processes\n",
    "        dist.barrier()\n",
    "    \n",
    "    def save_embeddings(self, path: str):\n",
    "        \"\"\"Save the embedding layer state\"\"\"\n",
    "        if self.rank == 0:\n",
    "            torch.save(self.embedding_storage.module.state_dict(), path)\n",
    "    \n",
    "    def load_embeddings(self, path: str):\n",
    "        \"\"\"Load the embedding layer state\"\"\"\n",
    "        state_dict = torch.load(path, map_location=self.device)\n",
    "        self.embedding_storage.module.load_state_dict(state_dict)\n",
    "        dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize distributed environment if not already initialized\n",
      "Initialize distributed training environment\n"
     ]
    }
   ],
   "source": [
    "item_indices = list(items_dict.keys())\n",
    "meta_keys = ['main_category',\n",
    " 'title',\n",
    " 'subtitle',\n",
    " 'author',\n",
    " 'average_rating',\n",
    " 'features',\n",
    " 'description',\n",
    " 'categories',]\n",
    "num_embeddings = len(item_indices)\n",
    "\n",
    "# Initialize generator\n",
    "generator = DistributedEmbeddingGenerator(\n",
    "    model_name='allenai/longformer-base-4096',\n",
    "    batch_size=8,\n",
    "    num_embeddings=100000,\n",
    "    embedding_dim=768\n",
    ")\n",
    "\n",
    "generator.set_dataset_and_dataloader(items_dict,\n",
    "                                     item_indices,\n",
    "                                     meta_keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store embeddings\n",
    "generator.generate_and_store_embeddings(items_dict, item_indices, meta_keys)\n",
    "\n",
    "# Save embeddings (only on rank 0)\n",
    "if dist.get_rank() == 0:\n",
    "    generator.save_embeddings('embeddings.pt')\n",
    "            \n",
    "cleanup_distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_text_Books(meta: dict, keys: list):\n",
    "    text = \"\"\n",
    "    for key in keys:\n",
    "        try:\n",
    "            if isinstance(meta[key],str):\n",
    "                text += f\"{key}: {meta[key]}</s>\"\n",
    "            elif isinstance(meta[key],list):\n",
    "                if len(meta[key])>0:\n",
    "                    add = \", \".join(meta[key])\n",
    "                    text += f\"{key}: {add}</s>\"\n",
    "            if key=='author':\n",
    "                text += f\"author: {meta['author']['name']}</s>\"\n",
    "        except:\n",
    "            text += f\"{key}: None</s>\"\n",
    "    return text\n",
    "\n",
    "keys = ['main_category',\n",
    " 'title',\n",
    " 'subtitle',\n",
    " 'author',\n",
    " 'average_rating',\n",
    " 'features',\n",
    " 'description',\n",
    " 'categories',]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "texts = [get_text_Books(item,keys) for item in items_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(text) for text in texts])\n",
    "idx = np.argmax(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/278012 [00:01<102:47:19,  1.33s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 578.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 556.69 MiB is free. Process 2455427 has 78.57 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 965.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m main_category\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBooks\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m embedding_system \u001b[39m=\u001b[39m ItemEmbeddingSystem(num_items, max_length, device)\n\u001b[0;32m--> 154\u001b[0m embedding_system\u001b[39m.\u001b[39;49minitialize_embeddings(items_dict, main_category, batch_size)\n",
      "Cell \u001b[0;32mIn[9], line 85\u001b[0m, in \u001b[0;36mItemEmbeddingSystem.initialize_embeddings\u001b[0;34m(self, items_dict, main_category, batch_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     77\u001b[0m     batch_texts,\n\u001b[1;32m     78\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     82\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     84\u001b[0m \u001b[39m# Get embeddings\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m batch_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlongformer(\n\u001b[1;32m     86\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded,\n\u001b[1;32m     87\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m )\u001b[39m.\u001b[39mpooler_output\n\u001b[1;32m     90\u001b[0m \u001b[39m# Update embeddings\u001b[39;00m\n\u001b[1;32m     91\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(items_dict\u001b[39m.\u001b[39mkeys())[batch_start:batch_start \u001b[39m+\u001b[39m batch_size]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1729\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1721\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1722\u001b[0m     :, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :\n\u001b[1;32m   1723\u001b[0m ]\n\u001b[1;32m   1725\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1726\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, token_type_ids\u001b[39m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds\n\u001b[1;32m   1727\u001b[0m )\n\u001b[0;32m-> 1729\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1730\u001b[0m     embedding_output,\n\u001b[1;32m   1731\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1732\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1733\u001b[0m     padding_len\u001b[39m=\u001b[39;49mpadding_len,\n\u001b[1;32m   1734\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1735\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1736\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1737\u001b[0m )\n\u001b[1;32m   1738\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1739\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1309\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1298\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1299\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1300\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         output_attentions,\n\u001b[1;32m   1307\u001b[0m     )\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1309\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1310\u001b[0m         hidden_states,\n\u001b[1;32m   1311\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1312\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mhead_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1313\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1314\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1315\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1316\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1317\u001b[0m     )\n\u001b[1;32m   1318\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1237\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   1228\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ):\n\u001b[0;32m-> 1237\u001b[0m     self_attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m   1238\u001b[0m         hidden_states,\n\u001b[1;32m   1239\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1240\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1241\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1242\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1243\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1244\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1245\u001b[0m     )\n\u001b[1;32m   1246\u001b[0m     attn_output \u001b[39m=\u001b[39m self_attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1247\u001b[0m     outputs \u001b[39m=\u001b[39m self_attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1173\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   1164\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1172\u001b[0m ):\n\u001b[0;32m-> 1173\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m   1174\u001b[0m         hidden_states,\n\u001b[1;32m   1175\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1176\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1177\u001b[0m         is_index_masked\u001b[39m=\u001b[39;49mis_index_masked,\n\u001b[1;32m   1178\u001b[0m         is_index_global_attn\u001b[39m=\u001b[39;49mis_index_global_attn,\n\u001b[1;32m   1179\u001b[0m         is_global_attn\u001b[39m=\u001b[39;49mis_global_attn,\n\u001b[1;32m   1180\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1181\u001b[0m     )\n\u001b[1;32m   1182\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m   1183\u001b[0m     outputs \u001b[39m=\u001b[39m (attn_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:562\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m query_vectors \u001b[39m=\u001b[39m query_vectors\u001b[39m.\u001b[39mview(seq_len, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    560\u001b[0m key_vectors \u001b[39m=\u001b[39m key_vectors\u001b[39m.\u001b[39mview(seq_len, batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 562\u001b[0m attn_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sliding_chunks_query_key_matmul(\n\u001b[1;32m    563\u001b[0m     query_vectors, key_vectors, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_sided_attn_window_size\n\u001b[1;32m    564\u001b[0m )\n\u001b[1;32m    566\u001b[0m \u001b[39m# values to pad for attention probs\u001b[39;00m\n\u001b[1;32m    567\u001b[0m remove_from_windowed_attention_mask \u001b[39m=\u001b[39m (attention_mask \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:837\u001b[0m, in \u001b[0;36mLongformerSelfAttention._sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    834\u001b[0m diagonal_chunked_attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbcxd,bcyd->bcxy\u001b[39m\u001b[39m\"\u001b[39m, (query, key))  \u001b[39m# multiply\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39m# convert diagonals into columns\u001b[39;00m\n\u001b[0;32m--> 837\u001b[0m diagonal_chunked_attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pad_and_transpose_last_two_dims(\n\u001b[1;32m    838\u001b[0m     diagonal_chunked_attention_scores, padding\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    839\u001b[0m )\n\u001b[1;32m    841\u001b[0m \u001b[39m# allocate space for the overall attention matrix where the chunks are combined. The last dimension\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[39m# window_overlap previous words). The following column is attention score from each word to itself, then\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[39m# followed by window_overlap columns for the upper triangle.\u001b[39;00m\n\u001b[1;32m    846\u001b[0m diagonal_attention_scores \u001b[39m=\u001b[39m diagonal_chunked_attention_scores\u001b[39m.\u001b[39mnew_zeros(\n\u001b[1;32m    847\u001b[0m     (batch_size \u001b[39m*\u001b[39m num_heads, chunks_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, window_overlap, window_overlap \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    848\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:695\u001b[0m, in \u001b[0;36mLongformerSelfAttention._pad_and_transpose_last_two_dims\u001b[0;34m(hidden_states_padded, padding)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    693\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pad_and_transpose_last_two_dims\u001b[39m(hidden_states_padded, padding):\n\u001b[1;32m    694\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"pads rows and then flips rows and columns\"\"\"\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m     hidden_states_padded \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    696\u001b[0m         hidden_states_padded, padding\n\u001b[1;32m    697\u001b[0m     )  \u001b[39m# padding value is not important because it will be overwritten\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     hidden_states_padded \u001b[39m=\u001b[39m hidden_states_padded\u001b[39m.\u001b[39mview(\n\u001b[1;32m    699\u001b[0m         \u001b[39m*\u001b[39mhidden_states_padded\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], hidden_states_padded\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), hidden_states_padded\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states_padded\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:4552\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   4546\u001b[0m             \u001b[39m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4547\u001b[0m             \u001b[39m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4548\u001b[0m             \u001b[39m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4549\u001b[0m             \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m'\u001b[39m\u001b[39mtorch._decomp.decompositions\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39m_replication_pad(\n\u001b[1;32m   4550\u001b[0m                 \u001b[39minput\u001b[39m, pad\n\u001b[1;32m   4551\u001b[0m             )\n\u001b[0;32m-> 4552\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, pad, mode, value)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 578.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 556.69 MiB is free. Process 2455427 has 78.57 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 965.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class ItemEmbeddingSystem(nn.Module):\n",
    "    def __init__(self, num_items: int, max_length: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        set_seed(42)    \n",
    "        self.num_items = num_items\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        # Initialize Longformer for text processing\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.longformer = AutoModel.from_pretrained('allenai/longformer-base-4096').to(self.device)\n",
    "        self.embedding_dim = self.longformer.config.hidden_size\n",
    "        \n",
    "        # Create learnable embedding layer\n",
    "        # self.item_embeddings = nn.Embedding(self.num_items, self.embedding_dim)\n",
    "        self.item_embeddings = np.zeros((self.num_items,self.embedding_dim))\n",
    "        # self.item_embeddings = np.empty((self.num_items,self.embedding_dim))\n",
    "\n",
    "        # meta_keys\n",
    "        self.meta_keys = {\n",
    "            'Books':['title',\n",
    "                    'subtitle',\n",
    "                    'author',\n",
    "                    'categories',\n",
    "                    'average_rating',\n",
    "                    'features',\n",
    "                    # 'description',\n",
    "                    ]\n",
    "            }\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "    def create_item_text(self, item: Dict, main_category: str) -> str:\n",
    "        def get_text_Books(meta: Dict, keys: list):\n",
    "            text = \"\"\n",
    "            for key in keys:\n",
    "                try:\n",
    "                    if isinstance(meta[key],str):\n",
    "                        text += f\"{key}: {meta[key]}</s>\"\n",
    "                    elif isinstance(meta[key],list):\n",
    "                        if len(meta[key])>0:\n",
    "                            add = \", \".join(meta[key])\n",
    "                            text += f\"{key}: {add}</s>\"\n",
    "                    if key=='author':\n",
    "                        text += f\"author: {meta['author']['name']}</s>\"\n",
    "                except:\n",
    "                    text += f\"{key}: None</s>\"\n",
    "            return text\n",
    "        \n",
    "        if main_category==\"Books\":\n",
    "            keys = self.meta_keys['Books']\n",
    "            text = get_text_Books(item, keys)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def initialize_embeddings(self, items_dict: Dict[int, Dict], main_category, batch_size: int = 32):\n",
    "        print(\"Initializing embeddings...\")\n",
    "        \n",
    "        # Pre-process all texts at once\n",
    "        all_texts = [self.create_item_text(item, main_category) for item in items_dict.values()]\n",
    "        \n",
    "        # Process in larger batches\n",
    "        # batch_size = 128  # Increased batch size\n",
    "        \n",
    "        for batch_start in tqdm(range(0, len(all_texts), batch_size)):\n",
    "            batch_texts = all_texts[batch_start:batch_start + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encoded = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            batch_embedding = self.longformer(\n",
    "                **encoded,\n",
    "                return_dict=True\n",
    "            ).pooler_output\n",
    "            \n",
    "            # Update embeddings\n",
    "            indices = list(items_dict.keys())[batch_start:batch_start + batch_size]\n",
    "            self.item_embeddings[batch_start:batch_start+batch_size]+= batch_embedding.detach().cpu().numpy()\n",
    "            # self.item_embeddings[batch_start:batch_start+batch_size]= batch_embedding.detach().cpu().numpy()\n",
    "            # for idx, item_idx in enumerate(indices):\n",
    "                # self.item_embeddings.weight.data[int(item_idx)] = batch_embedding[idx]\n",
    "                \n",
    "\n",
    "    # def initialize_embeddings(self, items_dict: Dict[int, Dict], main_cateogry, batch_size: int = 32):\n",
    "    #     \"\"\"Initialize embeddings using Longformer in batches\"\"\"\n",
    "    #     print(\"Initializing embeddings with Longformer...\")\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         for batch_start in tqdm(range(0, len(items_dict), batch_size)):\n",
    "    #             batch_items = {\n",
    "    #                 idx: items_dict[idx] \n",
    "    #                 for idx in list(items_dict.keys())[batch_start:batch_start + batch_size]\n",
    "    #             }\n",
    "                \n",
    "    #             # Process batch\n",
    "    #             texts = [self.create_item_text(item, main_category) for item in batch_items.values()]\n",
    "    #             encoded = self.tokenizer(\n",
    "    #                 texts,\n",
    "    #                 padding=True,\n",
    "    #                 truncation=True,\n",
    "    #                 max_length=self.max_length,\n",
    "    #                 return_tensors='pt'\n",
    "    #             )\n",
    "                \n",
    "    #             # Move to device\n",
    "    #             input_ids = encoded['input_ids'].to(self.device)\n",
    "    #             attention_mask = encoded['attention_mask'].to(self.device)\n",
    "                \n",
    "    #             # Get Longformer embeddings\n",
    "    #             batch_embedding = self.longformer(\n",
    "    #                 input_ids=input_ids,\n",
    "    #                 attention_mask=attention_mask,\n",
    "    #                 return_dict=True\n",
    "    #             ).pooler_output\n",
    "                \n",
    "    #             # Update embedding layer\n",
    "    #             for idx,item_idx in enumerate(batch_items.keys()):\n",
    "    #                 self.item_embeddings.weight.data[int(item_idx)] = batch_embedding[idx]\n",
    "        \n",
    "\n",
    "    def forward(self, item_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for given item indices\"\"\"\n",
    "        return self.item_embeddings(item_indices)\n",
    "    \n",
    "    def save_embeddings(self, path: str):\n",
    "        \"\"\"Save embeddings to file\"\"\"\n",
    "        torch.save(self.item_embeddings.state_dict(), path)\n",
    "    \n",
    "    def load_embeddings(self, path: str):\n",
    "        \"\"\"Load embeddings from file\"\"\"\n",
    "        self.item_embeddings.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "num_items = len(items_dict)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "max_length = 1024\n",
    "main_category=\"Books\"\n",
    "embedding_system = ItemEmbeddingSystem(num_items, max_length, device)\n",
    "embedding_system.initialize_embeddings(items_dict, main_category, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
