{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('/home/doyooni303/experiments/LLMRec/ReLLMRec')\n",
    "os.chdir('/home/doyooni303/experiments/LLMRec/ReLLMRec')\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from src.dataset.dataset import AmazonDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--yaml', type=str, default='./configs/Books.yaml')\n",
    "args = parser.parse_args([])\n",
    "config = yaml.load(open(args.yaml, 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 1188598/1188598 [00:03<00:00, 344928.29it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset = AmazonDataset(config, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': tensor([393856,  71642]),\n",
       " 'user_item_ids': ['347517 423591 164271', '110501 97412 101513 283686'],\n",
       " 'ui_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009]],\n",
       "        device='cuda:2'),\n",
       " 'ui_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:2'),\n",
       " 'su_query_input_ids': tensor([[128000,   8586,    374,  ..., 128009, 128009, 128009],\n",
       "         [128000,   8586,    374,  ..., 128009, 128009, 128009]],\n",
       "        device='cuda:2'),\n",
       " 'su_query_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:2'),\n",
       " 'target_item_id': tensor([91337, 85772]),\n",
       " 'target_item_title': ['The Absolute Best Dump Cake Cookbook: More Than 60 Tasty Dump Cakes',\n",
       "  \"That's Not My Monster...(Usborne Touchy-Feely Books)\"]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{config[\"gpu\"]}' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "torch.random.manual_seed(config['seed'])\n",
    "trainloader = DataLoader(trainset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "batch = next(iter(trainloader))\n",
    "for k,v in batch.items():\n",
    "    if (\"input_ids\" in k) or ('attention_mask' in k):\n",
    "        batch[k] = v.to(device)\n",
    "    else:\n",
    "        batch[k] = v\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def find_top_k_item_embeddings(query_vectors, item_embeddings, device, batch_size=10000, k=5):\n",
    "#     # Ensure inputs are on the correct device\n",
    "#     if query_vectors.device != item_embeddings.device:\n",
    "#         query_vectors = query_vectors.to(device)\n",
    "#         item_embeddings = item_embeddings.to(device)\n",
    "    \n",
    "#     # Normalize the vectors for cosine similarity\n",
    "#     query_vectors_normalized = F.normalize(query_vectors, p=2, dim=1)\n",
    "#     item_embeddings_normalized = F.normalize(item_embeddings, p=2, dim=1)\n",
    "    \n",
    "#     # For large embedding matrices, compute similarity in batches\n",
    "#     num_batches = (item_embeddings.shape[0] + batch_size - 1) // batch_size\n",
    "    \n",
    "#     # Initialize variables to store results on the GPU\n",
    "#     num_queries = query_vectors.shape[0]\n",
    "#     top_k_similarities = torch.full((num_queries, k), -float('inf'), device=device)\n",
    "#     top_k_indices = torch.zeros((num_queries, k), dtype=torch.long, device=device)\n",
    "    \n",
    "#     # Compute similarities batch by batch\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, item_embeddings.shape[0])\n",
    "        \n",
    "#         # Compute cosine similarity for this batch\n",
    "#         batch_similarities = torch.matmul(\n",
    "#             query_vectors_normalized, \n",
    "#             item_embeddings_normalized[start_idx:end_idx].t()\n",
    "#         )\n",
    "        \n",
    "#         # If index 0 is in this batch, set its similarity to -inf to ignore it\n",
    "#         if start_idx == 0 and end_idx > 0:\n",
    "#             batch_similarities[:, 0] = -float('inf')\n",
    "        \n",
    "#         # For each query, get top-k similarities from this batch\n",
    "#         batch_top_values, batch_top_indices = torch.topk(batch_similarities, min(k, batch_similarities.shape[1]), dim=1)\n",
    "        \n",
    "#         # Adjust indices to account for batching\n",
    "#         batch_top_indices = batch_top_indices + start_idx\n",
    "        \n",
    "#         # Merge with existing top-k\n",
    "#         if i == 0:\n",
    "#             # For the first batch, just store the values\n",
    "#             top_k_similarities[:, :batch_top_values.shape[1]] = batch_top_values\n",
    "#             top_k_indices[:, :batch_top_indices.shape[1]] = batch_top_indices\n",
    "#         else:\n",
    "#             # For subsequent batches, merge and re-sort\n",
    "#             combined_similarities = torch.cat([top_k_similarities, batch_top_values], dim=1)\n",
    "#             combined_indices = torch.cat([top_k_indices, batch_top_indices], dim=1)\n",
    "            \n",
    "#             # Re-sort to get the overall top-k\n",
    "#             for q in range(num_queries):\n",
    "#                 q_top_values, q_top_indices = torch.topk(combined_similarities[q], k, dim=0)\n",
    "#                 top_k_similarities[q] = q_top_values\n",
    "#                 top_k_indices[q] = combined_indices[q][q_top_indices]\n",
    "        \n",
    "#         # Optional: Clear GPU cache periodically\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Final check to ensure index 0 isn't in the results\n",
    "#     for q in range(num_queries):\n",
    "#         # Check if index 0 is in the results\n",
    "#         zero_mask = top_k_indices[q] == 0\n",
    "#         if torch.any(zero_mask):\n",
    "#             # If found, replace with the next best item\n",
    "#             remaining_indices = torch.ones(item_embeddings.shape[0], dtype=torch.bool, device=device)\n",
    "#             # Mark indices already in top-k as used\n",
    "#             remaining_indices[top_k_indices[q]] = False\n",
    "#             # Also mark index 0 as used\n",
    "#             remaining_indices[0] = False\n",
    "            \n",
    "#             # Find the count of indices to replace\n",
    "#             num_to_replace = torch.sum(zero_mask).item()\n",
    "            \n",
    "#             if num_to_replace > 0 and torch.any(remaining_indices):\n",
    "#                 # Find remaining embeddings\n",
    "#                 remaining_embeddings = item_embeddings_normalized[remaining_indices]\n",
    "                \n",
    "#                 # Compute similarities for unused indices\n",
    "#                 remaining_similarities = torch.matmul(\n",
    "#                     query_vectors_normalized[q:q+1], \n",
    "#                     remaining_embeddings.t()\n",
    "#                 )[0]\n",
    "                \n",
    "#                 # Get top replacements\n",
    "#                 replacement_values, replacement_relative_indices = torch.topk(\n",
    "#                     remaining_similarities, \n",
    "#                     min(num_to_replace, torch.sum(remaining_indices).item())\n",
    "#                 )\n",
    "                \n",
    "#                 # Convert relative indices to absolute\n",
    "#                 replacement_indices = torch.nonzero(remaining_indices, as_tuple=True)[0][replacement_relative_indices]\n",
    "                \n",
    "#                 # Replace zeros in the results\n",
    "#                 zero_positions = torch.nonzero(zero_mask, as_tuple=True)[0]\n",
    "#                 for j in range(min(len(zero_positions), len(replacement_indices))):\n",
    "#                     top_k_indices[q, zero_positions[j]] = replacement_indices[j]\n",
    "    \n",
    "#     # Retrieve the actual embeddings using the indices\n",
    "#     # Shape: [batch_size, k, embedding_dim]\n",
    "#     top_k_item_embeddings = torch.zeros((num_queries, k, item_embeddings.shape[1]), device=device)\n",
    "#     for q in range(num_queries):\n",
    "#         top_k_item_embeddings[q] = item_embeddings[top_k_indices[q]]\n",
    "    \n",
    "#     return top_k_indices, top_k_item_embeddings\n",
    "\n",
    "# # # Define your device\n",
    "# # device = torch.device(f'cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # # Find the top-5 most similar items and their embeddings\n",
    "# # k = 5  # You specified k=5\n",
    "# # top_k_indices, top_k_item_embeddings = find_top_k_item_embeddings(A, B, device, k=k)\n",
    "\n",
    "# # # Print shapes to confirm\n",
    "# # print(f\"Shape of top_k_indices: {top_k_indices.shape}\")           # Should be [4, 5]\n",
    "# # print(f\"Shape of top_k_item_embeddings: {top_k_item_embeddings.shape}\")  # Should be [4, 5, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# top_k = 5\n",
    "# batch_size = 2\n",
    "# dim = 768\n",
    "# random.seed(config['seed'])\n",
    "# torch.random.manual_seed(config['seed'])\n",
    "\n",
    "# target_item_ids = torch.tensor([1,6,8,7])\n",
    "# target_item_embeddings = torch.randn(batch_size, dim)\n",
    "\n",
    "# top_k_indices = torch.tensor([random.sample(range(1,10), top_k) for _ in range(batch_size)])\n",
    "# top_k_item_embeddings = torch.randn(batch_size, top_k, dim)\n",
    "\n",
    "# perm_cols = torch.randperm(top_k)\n",
    "\n",
    "# for i in range(config['batch_size']):\n",
    "#     target = target_item_ids[i]\n",
    "#     if target in top_k_indices[i]:\n",
    "#         pass\n",
    "#     else:\n",
    "#         top_k_indices[i][-1] = target\n",
    "# perm_k_indices = top_k_indices[:,perm_cols]\n",
    "# perm_k_embeddings = top_k_item_embeddings[:, perm_cols, :]\n",
    "\n",
    "# target_positions = torch.empty(config['batch_size'], dtype=torch.long)\n",
    "# non_target_positions = torch.ones((config['batch_size'],config['top_k']), dtype=torch.bool)\n",
    "# for i, (target_id, indices ) in enumerate(zip(target_item_ids, perm_k_indices)):\n",
    "#     position = torch.where(target_id == indices)[0].item()\n",
    "#     target_positions[i] = position\n",
    "#     non_target_positions[i][position] = False\n",
    "    \n",
    "# linear = nn.Parameter(torch.randn(dim))\n",
    "# output = torch.matmul(perm_k_embeddings, linear)\n",
    "\n",
    "# ce_loss = F.cross_entropy(output, target_positions, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CandiRec(nn.Module):\n",
    "    def __init__(self, config: dict, llm: AutoModelForCausalLM, device: torch.device, dtype: torch.dtype = torch.float16):\n",
    "        super(CandiRec, self).__init__()\n",
    "        torch.random.manual_seed(config['seed'])\n",
    "        \n",
    "        self.config = config\n",
    "        self.llm = llm\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.set_parameters()\n",
    "\n",
    "    def set_parameters(self,):\n",
    "        self.item_embeddings = torch.load(os.path.join(config['path'], f\"{config['fname']}_item_embeddings.pt\"))\n",
    "        self.alignment_1 = nn.Linear(in_features=config['llm_dim'], out_features=config['latent_dim'])\n",
    "        self.alignment_2 = nn.Linear(in_features=config['latent_dim'], out_features=config['item_dim'])\n",
    "        self.alignment = nn.Sequential(self.alignment_1, self.alignment_2)\n",
    "        self.linear = nn.Parameter(torch.randn(config['item_dim']))\n",
    "\n",
    "\n",
    "    def find_top_k_item_embeddings(self, query_vectors, item_embeddings,batch_size=10000, k=5):\n",
    "        # Ensure inputs are on the correct device\n",
    "        if query_vectors.device != item_embeddings.device:\n",
    "            query_vectors = query_vectors.to(device)\n",
    "            item_embeddings = item_embeddings.to(device)\n",
    "        \n",
    "        # Normalize the vectors for cosine similarity\n",
    "        query_vectors_normalized = F.normalize(query_vectors, p=2, dim=1).to(dtype=self.dtype)\n",
    "        item_embeddings_normalized = F.normalize(item_embeddings, p=2, dim=1).to(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        # For large embedding matrices, compute similarity in batches\n",
    "        num_batches = (item_embeddings.shape[0] + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Initialize variables to store results on the GPU\n",
    "        num_queries = query_vectors.shape[0]\n",
    "        top_k_similarities = torch.full((num_queries, k), -float('inf'), device=device, dtype=self.dtype)\n",
    "        top_k_indices = torch.zeros((num_queries, k),device= self.device, dtype=torch.long, )\n",
    "        \n",
    "        # Compute similarities batch by batch\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, item_embeddings.shape[0])\n",
    "            \n",
    "            # Compute cosine similarity for this batch\n",
    "            batch_similarities = torch.matmul(\n",
    "                query_vectors_normalized, \n",
    "                item_embeddings_normalized[start_idx:end_idx].t()\n",
    "            )\n",
    "            \n",
    "            # If index 0 is in this batch, set its similarity to -inf to ignore it\n",
    "            if start_idx == 0 and end_idx > 0:\n",
    "                batch_similarities[:, 0] = -float('inf')\n",
    "            \n",
    "            # For each query, get top-k similarities from this batch\n",
    "            batch_top_values, batch_top_indices = torch.topk(batch_similarities, min(k, batch_similarities.shape[1]), dim=1)\n",
    "            \n",
    "            # Adjust indices to account for batching\n",
    "            batch_top_indices = batch_top_indices + start_idx\n",
    "            \n",
    "            # Merge with existing top-k\n",
    "            if i == 0:\n",
    "                # For the first batch, just store the values\n",
    "                top_k_similarities[:, :batch_top_values.shape[1]] = batch_top_values\n",
    "                top_k_indices[:, :batch_top_indices.shape[1]] = batch_top_indices\n",
    "            else:\n",
    "                # For subsequent batches, merge and re-sort\n",
    "                combined_similarities = torch.cat([top_k_similarities, batch_top_values], dim=1)\n",
    "                combined_indices = torch.cat([top_k_indices, batch_top_indices], dim=1)\n",
    "                \n",
    "                # Re-sort to get the overall top-k\n",
    "                for q in range(num_queries):\n",
    "                    q_top_values, q_top_indices = torch.topk(combined_similarities[q], k, dim=0)\n",
    "                    top_k_similarities[q] = q_top_values\n",
    "                    top_k_indices[q] = combined_indices[q][q_top_indices]\n",
    "            \n",
    "            # Optional: Clear GPU cache periodically\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Final check to ensure index 0 isn't in the results\n",
    "        for q in range(num_queries):\n",
    "            # Check if index 0 is in the results\n",
    "            zero_mask = top_k_indices[q] == 0\n",
    "            if torch.any(zero_mask):\n",
    "                # If found, replace with the next best item\n",
    "                remaining_indices = torch.ones(item_embeddings.shape[0], dtype=torch.bool, device=self.device)\n",
    "                # Mark indices already in top-k as used\n",
    "                remaining_indices[top_k_indices[q]] = False\n",
    "                # Also mark index 0 as used\n",
    "                remaining_indices[0] = False\n",
    "                \n",
    "                # Find the count of indices to replace\n",
    "                num_to_replace = torch.sum(zero_mask).item()\n",
    "                \n",
    "                if num_to_replace > 0 and torch.any(remaining_indices):\n",
    "                    # Find remaining embeddings\n",
    "                    remaining_embeddings = item_embeddings_normalized[remaining_indices]\n",
    "                    \n",
    "                    # Compute similarities for unused indices\n",
    "                    remaining_similarities = torch.matmul(\n",
    "                        query_vectors_normalized[q:q+1], \n",
    "                        remaining_embeddings.t()\n",
    "                    )[0]\n",
    "                    \n",
    "                    # Get top replacements\n",
    "                    replacement_values, replacement_relative_indices = torch.topk(\n",
    "                        remaining_similarities, \n",
    "                        min(num_to_replace, torch.sum(remaining_indices).item())\n",
    "                    )\n",
    "                    \n",
    "                    # Convert relative indices to absolute\n",
    "                    replacement_indices = torch.nonzero(remaining_indices, as_tuple=True)[0][replacement_relative_indices]\n",
    "                    \n",
    "                    # Replace zeros in the results\n",
    "                    zero_positions = torch.nonzero(zero_mask, as_tuple=True)[0]\n",
    "                    for j in range(min(len(zero_positions), len(replacement_indices))):\n",
    "                        top_k_indices[q, zero_positions[j]] = replacement_indices[j]\n",
    "        \n",
    "        # Retrieve the actual embeddings using the indices\n",
    "        # Shape: [batch_size, k, embedding_dim]\n",
    "        top_k_item_embeddings = torch.zeros((num_queries, k, item_embeddings.shape[1]), device=device, dtype=torch.float16)\n",
    "        for q in range(num_queries):\n",
    "            top_k_item_embeddings[q] = item_embeddings[top_k_indices[q]]\n",
    "        \n",
    "        return top_k_indices, top_k_item_embeddings\n",
    "    \n",
    "    \n",
    "    def forward(self, batch: dict):\n",
    "        torch.random.manual_seed(config['seed'])           \n",
    "\n",
    "        # Target Item embeddings: [batch_size, item_dim]\n",
    "        target_item_embeddings = self.item_embeddings[batch['target_item_id']]\n",
    "\n",
    "        # User embeddings: [batch_size, item_dim]\n",
    "        user_embeddings = torch.zeros(self.config['batch_size'], self.config['item_dim']).to(self.device)\n",
    "        for i, ids in enumerate(batch['user_item_ids']):\n",
    "            item_ids = [int(iid) for iid in ids.split(\" \")]\n",
    "            user_embeddings[i] += self.item_embeddings[item_ids].mean(dim=0)    \n",
    "\n",
    "        # Get Query Vectors\n",
    "        # output = self.llm(input_ids = batch['ui_query_input_ids'], attention_mask = batch['ui_query_attention_mask'])\n",
    "        # print(output.logits.shape)\n",
    "        \n",
    "        item_history_query = self.llm(input_ids = batch['ui_query_input_ids'], attention_mask = batch['ui_query_attention_mask'], output_hidden_states=True).hidden_states[-1][:,-1,:]\n",
    "        similar_user_query = self.llm(input_ids = batch['su_query_attention_mask'],attention_mask = batch['su_query_attention_mask'], output_hidden_states=True).hidden_states[-1][:,-1,:]\n",
    "        \n",
    "        query_embedding = self.alignment(item_history_query + similar_user_query)\n",
    "\n",
    "        # Get Item Embeddings\n",
    "        top_k_indices, top_k_item_embeddings = self.find_top_k_item_embeddings(query_embedding, self.item_embeddings, k=config['top_k'])\n",
    "        # print(top_k_item_embeddings.shape, top_k_item_embeddings.dtype)\n",
    "        perm_cols = torch.randperm(self.config['top_k'])\n",
    "        for i, target in enumerate(batch['target_item_id']):\n",
    "            if target in top_k_indices[i]:\n",
    "                pass\n",
    "            else:\n",
    "                top_k_indices[i][-1] = target\n",
    "        \n",
    "        perm_k_indices = top_k_indices[:,perm_cols]\n",
    "        # perm_target_item_id = batch['target_item_id'][perm_cols]\n",
    "        perm_k_item_embeddings = top_k_item_embeddings[:, perm_cols, :]\n",
    "        \n",
    "        # Get Target Item positions\n",
    "        target_positions = torch.empty(self.config['batch_size'], dtype=torch.long, device=self.device)\n",
    "        non_target_positions = torch.ones((config['batch_size'],config['top_k']), dtype=torch.bool, device=self.device)\n",
    "        for i, (target_id, indices ) in enumerate(zip(batch['target_item_id'], perm_k_indices)):\n",
    "            position = torch.where(target_id == indices)[0].item()\n",
    "            target_positions[i] = position\n",
    "            non_target_positions[i][position] = False\n",
    "        \n",
    "        # Get Cross Entropy Loss\n",
    "        # print(torch.matmul(perm_k_item_embeddings, self.linear).shape, torch.matmul(perm_k_item_embeddings, self.linear).dtype)\n",
    "        # print(target_positions.shape, target_positions.dtype)\n",
    "        ce_loss = F.cross_entropy(torch.matmul(perm_k_item_embeddings, self.linear), target_positions)\n",
    "\n",
    "        # Get Contrastive loss\n",
    "        negative_item_embeddings = perm_k_item_embeddings[non_target_positions].reshape(self.config['batch_size'], -1, self.config['item_dim']).transpose(1,2)\n",
    "        cl_loss = (1-F.sigmoid(torch.bmm(target_item_embeddings.unsqueeze(1),negative_item_embeddings).squeeze(1))).mean(1)\n",
    "\n",
    "        self.loss = ce_loss + cl_loss\n",
    "\n",
    "        return top_k_indices, self.loss\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65377cda11b4301acbdaff5e6254952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{config[\"gpu\"]}' if torch.cuda.is_available() else 'cpu')\n",
    "llm = AutoModelForCausalLM.from_pretrained(config['model_name'], torch_dtype=torch.float16, device_map=device)\n",
    "lora_config = LoraConfig(task_type=getattr(TaskType, config['LoRA TaskType']),\n",
    "                         **config['LoRA'])\n",
    "llm = get_peft_model(llm, lora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CandiRec(config, llm, device).to(device=device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 242\u001b[0m, in \u001b[0;36mCandiRec.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    239\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_llm_query_embeddings(batch)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Prepare candidate items\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m perm_k_indices, perm_k_item_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Calculate target positions\u001b[39;00m\n\u001b[1;32m    245\u001b[0m target_positions, non_target_positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_target_positions(\n\u001b[1;32m    246\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_item_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    247\u001b[0m     perm_k_indices\n\u001b[1;32m    248\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 196\u001b[0m, in \u001b[0;36mCandiRec._prepare_candidates\u001b[0;34m(self, query_embedding, batch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepare candidate items\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Find top k items\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m top_k_indices, top_k_item_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_top_k_item_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Add target item if not present\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_item_id\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[8], line 120\u001b[0m, in \u001b[0;36mCandiRec.find_top_k_item_embeddings\u001b[0;34m(self, query_vectors, item_embeddings, batch_size, k)\u001b[0m\n\u001b[1;32m    113\u001b[0m top_k_item_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    114\u001b[0m     (num_queries, k, item_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), \n\u001b[1;32m    115\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \n\u001b[1;32m    116\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_queries):\n\u001b[0;32m--> 120\u001b[0m     top_k_item_embeddings[q] \u001b[38;5;241m=\u001b[39m \u001b[43mitem_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_k_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m top_k_indices, top_k_item_embeddings\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "model(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
