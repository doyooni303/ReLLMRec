{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "fname=\"Books\"\n",
    "path = \"/DATA/Recommendation/amazon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def open_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        dict_list = [json.loads(line[:-1]) for line in f.readlines()]\n",
    "    return dict_list\n",
    "meta = open_json(os.path.join(path, f\"meta_{fname}.jsonl\"))\n",
    "data = open_json(os.path.join(path, f\"{fname}.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1150206/1150206 [00:05<00:00, 218523.57it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open(os.path.join(path, f\"train.txt\"), \"r\").readlines()\n",
    "train = dict()\n",
    "for line in tqdm(txt):\n",
    "    u, history = line.rstrip().split(\":\")\n",
    "    history = [int(i) for i in history.split(\" \")]\n",
    "    u = int(u)\n",
    "    train.update({u: history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150206"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29475453/29475453 [00:48<00:00, 610428.03it/s]\n",
      "100%|██████████| 29475453/29475453 [02:56<00:00, 166902.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Books_meta_name_dict.json\n",
      "1206660 980848\n",
      "Saved Books.txt\n"
     ]
    }
   ],
   "source": [
    "def get_path(fname: str, ftype: str, folder: str):\n",
    "    file_path = os.path.join(folder, f\"{fname}.{ftype}\")\n",
    "    meta_path = os.path.join(folder, f\"meta_{fname}.{ftype}\")\n",
    "    return file_path, meta_path\n",
    "\n",
    "def preprocess(fname, ftype=\"jsonl\", folder=\"/DATA/Recommendation/amazon\", save=False, data_list=None, meta_list=None):\n",
    "    countU = defaultdict(lambda: 0)\n",
    "    countP = defaultdict(lambda: 0)\n",
    "    line = 0\n",
    "\n",
    "    usermap = dict()\n",
    "    usernum = 0\n",
    "    itemmap = dict()\n",
    "    itemnum = 0\n",
    "    User = dict()\n",
    "    # review_dict = {}\n",
    "    # name_dict = {\"title\": {}, \"description\": {}}\n",
    "    if fname == \"Books\":\n",
    "        name_dict = {\n",
    "            \"title\": {},\n",
    "            \"author\": {},\n",
    "            \"average_rating\": {},\n",
    "            \"features\": {},\n",
    "            \"categories\": {},\n",
    "        }\n",
    "\n",
    "    file_path, meta_path = get_path(fname, ftype, folder)\n",
    "\n",
    "    if data_list is None:\n",
    "        print(f\"Loading jsonl files..\")\n",
    "        data_list = open_json(file_path)\n",
    "    if meta_list is None:\n",
    "        meta_list = open_json(meta_path)\n",
    "\n",
    "\n",
    "    if ftype == \"jsonl\":\n",
    "        # counting interactions for each user and item\n",
    "\n",
    "        for l in tqdm(data_list):\n",
    "            line += 1\n",
    "            asin = l[\"asin\"]\n",
    "            pasin = l[\"parent_asin\"]\n",
    "            rev = l[\"user_id\"]\n",
    "            time = l[\"timestamp\"]\n",
    "            countU[rev] += 1\n",
    "            countP[asin] += 1\n",
    "\n",
    "        meta_dict = {}\n",
    "        for l in meta_list:\n",
    "            meta_dict[l[\"parent_asin\"]] = l\n",
    "\n",
    "        for l in tqdm(data_list):\n",
    "            line += 1\n",
    "            asin = l[\"asin\"]\n",
    "            pasin = l[\"parent_asin\"]\n",
    "            rev = l[\"user_id\"]\n",
    "            time = l[\"timestamp\"]\n",
    "\n",
    "            threshold = 5\n",
    "            if (\"Beauty\" in fname) or (\"Toys\" in fname):\n",
    "                threshold = 4\n",
    "\n",
    "            if countU[rev] < threshold or countP[asin] < threshold:\n",
    "                continue\n",
    "\n",
    "            if rev in usermap:\n",
    "                userid = usermap[rev]\n",
    "            else:\n",
    "                usernum += 1\n",
    "                userid = usernum\n",
    "                usermap[rev] = userid\n",
    "                User[userid] = []\n",
    "\n",
    "            if asin in itemmap:\n",
    "                itemid = itemmap[asin]\n",
    "            else:\n",
    "                itemnum += 1\n",
    "                itemid = itemnum\n",
    "                itemmap[asin] = itemid\n",
    "            \n",
    "            User[userid].append([time, itemid])\n",
    "\n",
    "            \n",
    "            for key in name_dict.keys():\n",
    "\n",
    "                if key in [\"title\", \"average_rating\"]:\n",
    "                    name_dict[key][itemmap[asin]] = meta_dict[pasin].get(key, \"None\")\n",
    "\n",
    "                elif key == \"author\":\n",
    "                    try:\n",
    "                        author_name = meta_dict[pasin][key].get(\"name\", \"Unknown\")\n",
    "                        author_info = meta_dict[pasin][key].get(\"about\", \"None\")\n",
    "                        if isinstance(author_info, list):\n",
    "                            author_info = \" \".join(author_info)\n",
    "                        name_dict[key][\n",
    "                            itemmap[asin]\n",
    "                        ] = f\"name: {author_name}, about: {author_info}\"\n",
    "                    except:\n",
    "                        name_dict[key][itemmap[asin]] = \"None\"\n",
    "                elif key == \"features\":\n",
    "                    features = meta_dict[pasin].get(key, \"None\")\n",
    "                    if isinstance(features, list):\n",
    "                        name_dict[key][itemmap[asin]] = \" \".join(features)\n",
    "                    else:\n",
    "                        name_dict[key][itemmap[asin]] = features\n",
    "\n",
    "                elif key == \"categories\":\n",
    "                    categories = meta_dict[pasin].get(key, \"None\")\n",
    "                    if isinstance(categories, list):\n",
    "                        name_dict[key][itemmap[asin]] = \", \".join(categories)\n",
    "                    else:\n",
    "                        name_dict[key][itemmap[asin]] = categories\n",
    "            \n",
    "    if save:\n",
    "        json.dump(\n",
    "            name_dict,\n",
    "            open(os.path.join(folder,f\"{fname}_meta_name_dict.json\"), \"w\"),\n",
    "        )\n",
    "        print(f\"Saved {fname}_meta_name_dict.json\")\n",
    "        for userid in User.keys():\n",
    "            User[userid].sort(key=lambda x: x[0])\n",
    "\n",
    "        print(usernum, itemnum)\n",
    "\n",
    "        f = open(os.path.join(folder, f\"{fname}.txt\"), \"w\", encoding=\"UTF-8\")\n",
    "        for user in User.keys():\n",
    "            for i in User[user]:\n",
    "                f.write(\"%d %d\\n\" % (user, i[1]))\n",
    "        f.close()\n",
    "        print(f\"Saved {fname}.txt\")\n",
    "\n",
    "        # usermap, itemmap save\n",
    "        for map_, name in zip([usermap, itemmap], [\"user\", \"item\"]):\n",
    "            with open(os.path.join(f\"{fname}_{name}map.json\"), \"w\") as f:\n",
    "                json.dump(map_, f)\n",
    "            print(f\"Saved {fname}_{name}map.json\")\n",
    "\n",
    "    \n",
    "    return User, countU, countP\n",
    "\n",
    "\n",
    "User, countU, countP = preprocess(fname=fname, ftype='jsonl', folder=path, data_list = data, meta_list=meta, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29475453/29475453 [00:50<00:00, 580894.26it/s]\n",
      "100%|██████████| 29475453/29475453 [04:21<00:00, 112625.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206660 980848\n",
      "Saved Books.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess(\n",
    "    fname,\n",
    "    ftype=\"jsonl\",\n",
    "    folder=\"/DATA/Recommendation/amazon\",\n",
    "    save=False,\n",
    "    data_list=None,\n",
    "    meta_list=None,\n",
    "):\n",
    "    countU = defaultdict(lambda: 0)\n",
    "    countP = defaultdict(lambda: 0)\n",
    "    line = 0\n",
    "\n",
    "    usermap = dict()\n",
    "    usernum = 0\n",
    "    itemmap = dict()\n",
    "    itemnum = 0\n",
    "    User = dict()\n",
    "    # review_dict = {}\n",
    "    # name_dict = {\"title\": {}, \"description\": {}}\n",
    "    if fname == \"Books\":\n",
    "        name_dict = {\n",
    "            \"title\": {},\n",
    "            \"author\": {},\n",
    "            \"average_rating\": {},\n",
    "            \"features\": {},\n",
    "            \"categories\": {},\n",
    "        }\n",
    "\n",
    "    file_path, meta_path = get_path(fname, ftype, folder)\n",
    "\n",
    "    if data_list is None:\n",
    "        print(f\"Loading jsonl files..\")\n",
    "        data_list = open_json(file_path)\n",
    "    if meta_list is None:\n",
    "        meta_list = open_json(meta_path)\n",
    "\n",
    "    if ftype == \"jsonl\":\n",
    "        # counting interactions for each user and item\n",
    "\n",
    "        for l in tqdm(data_list):\n",
    "            line += 1\n",
    "            asin = l[\"asin\"]\n",
    "            pasin = l[\"parent_asin\"]\n",
    "            rev = l[\"user_id\"]\n",
    "            time = l[\"timestamp\"]\n",
    "            countU[rev] += 1\n",
    "            countP[asin] += 1\n",
    "\n",
    "        meta_dict = {}\n",
    "        for l in meta_list:\n",
    "            meta_dict[l[\"parent_asin\"]] = l\n",
    "\n",
    "        for l in tqdm(data_list):\n",
    "            line += 1\n",
    "            asin = l[\"asin\"]\n",
    "            pasin = l[\"parent_asin\"]\n",
    "            rev = l[\"user_id\"]\n",
    "            time = l[\"timestamp\"]\n",
    "\n",
    "            threshold = 5\n",
    "            if (\"Beauty\" in fname) or (\"Toys\" in fname):\n",
    "                threshold = 4\n",
    "\n",
    "            if countU[rev] < threshold or countP[asin] < threshold:\n",
    "                continue\n",
    "\n",
    "            if rev in usermap:\n",
    "                userid = usermap[rev]\n",
    "            else:\n",
    "                usernum += 1\n",
    "                userid = usernum\n",
    "                usermap[rev] = userid\n",
    "                User[userid] = []\n",
    "\n",
    "            if asin in itemmap:\n",
    "                itemid = itemmap[asin]\n",
    "            else:\n",
    "                itemnum += 1\n",
    "                itemid = itemnum\n",
    "                itemmap[asin] = itemid\n",
    "\n",
    "            if [time, itemid] not in User[userid]:\n",
    "                User[userid].append([time, itemid])\n",
    "\n",
    "                for key in name_dict.keys():\n",
    "\n",
    "                    if key in [\"title\", \"average_rating\"]:\n",
    "                        name_dict[key][itemmap[asin]] = meta_dict[pasin].get(\n",
    "                            key, \"None\"\n",
    "                        )\n",
    "\n",
    "                    elif key == \"author\":\n",
    "                        try:\n",
    "                            author_name = meta_dict[pasin][key].get(\"name\", \"Unknown\")\n",
    "                            author_info = meta_dict[pasin][key].get(\"about\", \"None\")\n",
    "                            if isinstance(author_info, list):\n",
    "                                author_info = \" \".join(author_info)\n",
    "                            name_dict[key][\n",
    "                                itemmap[asin]\n",
    "                            ] = f\"name: {author_name}, about: {author_info}\"\n",
    "                        except:\n",
    "                            name_dict[key][itemmap[asin]] = \"None\"\n",
    "                    elif key == \"features\":\n",
    "                        features = meta_dict[pasin].get(key, \"None\")\n",
    "                        if isinstance(features, list):\n",
    "                            name_dict[key][itemmap[asin]] = \" \".join(features)\n",
    "                        else:\n",
    "                            name_dict[key][itemmap[asin]] = features\n",
    "\n",
    "                    elif key == \"categories\":\n",
    "                        categories = meta_dict[pasin].get(key, \"None\")\n",
    "                        if isinstance(categories, list):\n",
    "                            name_dict[key][itemmap[asin]] = \", \".join(categories)\n",
    "                        else:\n",
    "                            name_dict[key][itemmap[asin]] = categories\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    if save:\n",
    "        # json.dump(\n",
    "        #     name_dict,\n",
    "        #     open(os.path.join(folder, f\"{fname}_meta_name_dict.json\"), \"w\"),\n",
    "        # )\n",
    "        # print(f\"Saved {fname}_meta_name_dict.json\")\n",
    "        for userid in User.keys():\n",
    "            User[userid].sort(key=lambda x: x[0])\n",
    "\n",
    "        print(usernum, itemnum)\n",
    "\n",
    "        f = open(os.path.join(folder, f\"{fname}.txt\"), \"w\", encoding=\"UTF-8\")\n",
    "        for user in User.keys():\n",
    "            for i in User[user]:\n",
    "                f.write(\"%d %d\\n\" % (user, i[1]))\n",
    "        f.close()\n",
    "        print(f\"Saved {fname}.txt\")\n",
    "\n",
    "        # # usermap, itemmap save\n",
    "        # for map_, name in zip([usermap, itemmap], [\"user\", \"item\"]):\n",
    "        #     with open(os.path.join(f\"{fname}_{name}map.json\"), \"w\") as f:\n",
    "        #         json.dump(map_, f)\n",
    "        #     print(f\"Saved {fname}_{name}map.json\")\n",
    "\n",
    "preprocess(fname=fname, ftype='jsonl', folder=path, data_list = data, meta_list=meta, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. data partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 1206660/1206660 [00:01<00:00, 628042.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/doyooni303/experiments/LLMRec/ReLLMRec\")\n",
    "from src.dataset.utils import data_partition\n",
    "user_train, user_valid, user_test = data_partition(fname=fname, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books.txt exists\n",
      "splitting data by user: 100%|█████| 1206660/1206660 [00:04<00:00, 270228.33it/s]\n",
      "Preprocessing history: 100%|█████| 1206660/1206660 [00:00<00:00, 1341253.01it/s]\n",
      "Initializing UserSimilarityFinder...\n",
      "Finding similar users for 548840 query users...\n",
      "Get batches: 100%|████████████████████████| 536/536 [00:00<00:00, 101414.06it/s]\n",
      "Processing batches: 100%|█████████████████████| 536/536 [13:19<00:00,  1.49s/it]\n",
      "Saving results...\n",
      "All completed !\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/doyooni303/experiments/LLMRec/ReLLMRec/src/dataset/get_similar_users.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar user 구한 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "similar_users = json.load(open(os.path.join(path,f\"{fname}_similar_users.json\"), \"r\"))\n",
    "itemmap = json.load(open(os.path.join(path, f\"{fname}_itemmap.json\"), \"r\"))\n",
    "usermap = json.load(open(os.path.join(path, f\"{fname}_usermap.json\"), \"r\"))\n",
    "idx2user = {v: k for k, v in usermap.items()}\n",
    "idx2item = {v: k for k, v in itemmap.items()}\n",
    "\n",
    "# train, valid, test = data_partition(fname, \"/home/doyooni303/experiments/LLMRec/data/amazon/Books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12187673/12187673 [00:14<00:00, 860748.68it/s]\n",
      "splitting data by user: 100%|██████████| 1206660/1206660 [00:04<00:00, 256314.97it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open(os.path.join(path, f\"{fname}.txt\"), \"r\").readlines()\n",
    "\n",
    "USER = defaultdict(list)\n",
    "for line in tqdm(txt):\n",
    "    u, i = line.rstrip().split(\" \")\n",
    "    u = int(u)\n",
    "    i = int(i)\n",
    "    USER[u].append(i)\n",
    "\n",
    "# for SELFRec data save\n",
    "max_length = 20\n",
    "train,valid, test = [], [], []\n",
    "for user, h_list in tqdm(USER.items(), desc=\"splitting data by user\"):\n",
    "    if len(h_list) >= 3:\n",
    "        train.append(f\"{user}:{' '.join([str(i) for i in h_list[-(max_length+2):-2]])}\"+\"\\n\")\n",
    "        valid.append(f\"{user}:{h_list[-2]}\"+\"\\n\")\n",
    "        test.append(f\"{user}:{h_list[-1]}\\n\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "def write_text_file(file_path: str, data: List):\n",
    "  with open(file_path, 'w') as f:\n",
    "    f.writelines(data)\n",
    "\n",
    "write_text_file(os.path.join(path, \"train.txt\"), train)\n",
    "write_text_file(os.path.join(path, \"test.txt\"), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{693844: [\"Bloodborne Collector's Edition Strategy Guide\",\n",
       "  'The Poky Little Puppy (A Little Golden Book Classic)',\n",
       "  'The Monster at the End of This Book',\n",
       "  'Night of the Hunter: Companions Codex, I',\n",
       "  'The Companions: The Sundering, Book I'],\n",
       " 1058837: ['Baby Animals (Little Golden Book)',\n",
       "  'Dumbo (Disney Classic) (Little Golden Book)',\n",
       "  'Tawny Scrawny Lion (Little Golden Book)',\n",
       "  'The Little Red Hen (Little Golden Book)',\n",
       "  'The Poky Little Puppy (A Little Golden Book Classic)'],\n",
       " 135572: ['Galaxy S5 Case, GreatShield [TACT | Quill Design] SLIM FIT Pattern Rubber Coating Embossed Snap On Case Back Cover for Samsung Galaxy S5 SV (Gold)',\n",
       "  'The Three Bears',\n",
       "  'The Three Little Pigs (Disney Classic) (Little Golden Book)',\n",
       "  'The Poky Little Puppy (A Little Golden Book Classic)',\n",
       "  'The Monster at the End of This Book',\n",
       "  'The Phantom of the Opera'],\n",
       " 151622: ['Head, Shoulders, Knees and Toes...',\n",
       "  'The Pout-Pout Fish',\n",
       "  \"Giraffes Can't Dance (Board Book)\",\n",
       "  'The Wonderful Things You Will Be',\n",
       "  'The Monster at the End of This Book',\n",
       "  'Burnt Tongues',\n",
       "  'The Poky Little Puppy (A Little Golden Book Classic)'],\n",
       " 42270: ['Me, the Mob, and the Music: One Helluva Ride with Tommy James & The Shondells',\n",
       "  'The Poky Little Puppy (A Little Golden Book Classic)',\n",
       "  'The Monster at the End of This Book',\n",
       "  'The Victorian Home: The Grandeur and Comforts of the Victorian Era, in Households Past and Present',\n",
       "  'Complete Portrait Manual',\n",
       "  \"The Big Bad Book of Bill Murray: A Critical Appreciation of the World's Finest Actor\",\n",
       "  'Air Fryer Cookbook: 200 Outstanding, Unbelievable And Fantastic Recipes For Your Air Fryer',\n",
       "  'Digging Up Mother']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_name_dict = json.load(open(os.path.join(path, f\"{fname}_meta_name_dict.json\"), \"r\"))\n",
    "query_user_idx, _ = list(user_train.items())[10]\n",
    "\n",
    "query_similar_users = similar_users.get(str(query_user_idx), \"None\")\n",
    "if isinstance(query_similar_users, list):\n",
    "    query_similar_users = [idx for idx, _ in query_similar_users]\n",
    "\n",
    "similar_users_history = {}\n",
    "for idx in query_similar_users:\n",
    "    sequence = user_train.get(idx, [])\n",
    "    \n",
    "    sequence_titles = [meta_name_dict['title'][str(i)] for i in sequence]\n",
    "    similar_users_history.update({idx: sequence_titles})\n",
    "similar_users_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
