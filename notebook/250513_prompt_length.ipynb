{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"/home/doyooni303/experiments/recsys/ReLLMRec\")\n",
    "import argparse\n",
    "import yaml\n",
    "import hashlib\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from src.models import CandiRec\n",
    "from src.train import Trainer\n",
    "from src.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--path\", type=str, default=\"/data3/doyoon/sequential_recsys/datasets/amazon/Movies_and_TV/\")\n",
    "parser.add_argument(\"--type\", type=str, default=\"Original\")\n",
    "parser.add_argument(\"--fname\", type=str, default=\"Movies_and_TV\")\n",
    "parser.add_argument(\"--gpu\", type=str, default=\"0,1,2,3\", help=\"GPU ids to use (comma separated)\")\n",
    "parser.add_argument(\"--top_k\", type=int, default=10)\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=10)\n",
    "parser.add_argument(\"--max_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--train_mode\", type=str, default=\"epochs\", help=\"[epochs, steps]\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES based on gpu argument\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "yaml_path = os.path.join(\"/home/doyooni303/experiments/recsys/ReLLMRec/configs\", f\"{args.fname}.yaml\")\n",
    "config = yaml.load(open(yaml_path, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "for key, value in args.__dict__.items():\n",
    "    config.update({key: value})\n",
    "\n",
    "if config[\"train_mode\"] == \"epochs\":\n",
    "    name = f\"-\".join([f\"{key}_{config[key]}\" for key in [\"top_k\", \"max_epochs\"]])\n",
    "    config.update({\"eval_interval\": config[\"max_epochs\"] // 10})\n",
    "elif config[\"train_mode\"] == \"steps\":\n",
    "    name = f\"-\".join([f\"{key}_{config[key]}\" for key in [\"top_k\", \"max_steps\"]])\n",
    "    config.update({\"eval_interval\": config[\"max_steps\"] // 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data by user: 100%|██████████| 716592/716592 [00:03<00:00, 215493.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "from src.dataset import AmazonDataset\n",
    "train_dataset = AmazonDataset(config, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 657.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "uiq,suq = [],[]\n",
    "uiq_tok, suq_tok = [],[]\n",
    "for i in tqdm(range(10000)):\n",
    "    user_id = train_dataset.usermap[i]\n",
    "    data = train_dataset.data\n",
    "    meta_name_dict = train_dataset.meta_name_dict\n",
    "    tokenizer = train_dataset.tokenizer\n",
    "    similar_users = train_dataset.similar_users\n",
    "\n",
    "    user_item_query = train_dataset._format_item_list_query(user_id, data, meta_name_dict)\n",
    "    similar_users_query = train_dataset._format_similar_users_query(user_id, data, meta_name_dict, similar_users)\n",
    "    ui_tok = tokenizer(user_item_query, return_tensors='np')\n",
    "    su_tok = tokenizer(similar_users_query, return_tensors='np')\n",
    "    uiq.append(user_item_query)\n",
    "    suq.append(similar_users_query)\n",
    "    uiq_tok.append(ui_tok['input_ids'].shape[1])\n",
    "    suq_tok.append(su_tok['input_ids'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([105., 150., 224., 407., 699., 898.]),\n",
       " array([106.  , 113.  , 122.  , 159.  , 265.01, 584.  ]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.percentile(uiq_tok, q=[0,25,50,75,99,100]), np.percentile(suq_tok, q=[0,25,50,75,99,100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
