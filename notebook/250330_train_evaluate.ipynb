{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, yaml\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--yaml', type=str, default='./configs/Books.yaml')\n",
    "args = parser.parse_args([])\n",
    "config = yaml.load(open(args.yaml, 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "meta = json.load(open('/home/doyooni303/experiments/LLMRec/data/amazon/Books/Books_meta_name_dict.json','r'\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in meta.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('/home/doyooni303/experiments/LLMRec/ReLLMRec')\n",
    "os.chdir('/home/doyooni303/experiments/LLMRec/ReLLMRec')\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from src.dataset.dataset import AmazonDataset\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--yaml', type=str, default='./configs/Books.yaml')\n",
    "args = parser.parse_args([])\n",
    "config = yaml.load(open(args.yaml, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "device = torch.device(f'cuda:{config[\"gpu\"]}' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "torch.random.manual_seed(config['seed'])\n",
    "sys.path.append(os.getcwd())\n",
    "from src.models.model import CandiRec\n",
    "\n",
    "model = CandiRec(config).to(device=device, dtype=torch.float16)\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=float(config['lr']))\n",
    "validloader = DataLoader(AmazonDataset(config, 'valid'), batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "batch = next(iter(validloader))\n",
    "for k,v in batch.items():\n",
    "    if (\"input_ids\" in k) or ('attention_mask' in k):\n",
    "        batch[k] = v.to(device)\n",
    "    else:\n",
    "        batch[k] = v\n",
    "\n",
    "top_k_indices, preds = model(batch, flag='valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = batch['target_item_id'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k_indices = model.max_k_indices.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def hits(target_ids, top_k_ids):\n",
    "        if len(top_k_ids.shape) == 1:\n",
    "            top_k_ids = top_k_ids.reshape(1, -1)\n",
    "        hit_count = 0\n",
    "        for target_id, top_k_id in zip(target_ids, top_k_ids):\n",
    "            if target_id in top_k_id:\n",
    "                hit_count += 1\n",
    "        return hit_count\n",
    "\n",
    "    @staticmethod\n",
    "    def hit_ratio(target_ids, top_k_ids):\n",
    "        hit_count = Metric.hits(target_ids, top_k_ids)\n",
    "        return round(hit_count / len(target_ids), 5)\n",
    "\n",
    "    @staticmethod\n",
    "    def precision(taret_ids, top_k_ids, K):\n",
    "        hit_ratio = Metric.hit_ratio(taret_ids, top_k_ids)\n",
    "        return round(hit_ratio / K, 5)\n",
    "\n",
    "    @staticmethod\n",
    "    def recall(taret_ids, top_k_ids):\n",
    "        return Metric.hit_ratio(taret_ids, top_k_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def F1(taret_ids, top_k_ids, K):\n",
    "        prec = Metric.precision(taret_ids, top_k_ids, K)\n",
    "        recall = Metric.recall(taret_ids, top_k_ids)\n",
    "\n",
    "        if (prec + recall) != 0:\n",
    "            return round(2 * prec * recall / (prec + recall), 5)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def NDCG(target_ids, top_k_ids, K):\n",
    "        expanded_targets = np.expand_dims(target_ids, axis=1)\n",
    "        hit_matrix = (top_k_ids == expanded_targets).astype(np.float64)\n",
    "        position_indices = np.arange(1, K + 1)\n",
    "        discounts = 1 / np.log2(position_indices + 1)\n",
    "\n",
    "        dcg = np.sum(hit_matrix * discounts, axis=1)\n",
    "        idcg = 1 / np.log2(2)\n",
    "        ndcg = dcg / idcg\n",
    "\n",
    "        return ndcg, ndcg.mean()\n",
    "\n",
    "\n",
    "def ranking_evaluation(target_ids, max_k_ids, top_k_list: Union[list, int]):\n",
    "    measure = dict()\n",
    "    for K in top_k_list:\n",
    "        if K > max_k_ids.shape[1]:\n",
    "            K = max_k_ids.shape[1]\n",
    "            print(f\"K is larger than the number of items. K is set to {K}\")\n",
    "        top_k_ids = max_k_ids[:, :K]\n",
    "        hr = Metric.hit_ratio(target_ids, top_k_ids)\n",
    "        NDCG = Metric.NDCG(target_ids, top_k_ids, K)\n",
    "        measure.update({K: {\"HR\": hr, \"NDCG\": NDCG}})\n",
    "\n",
    "    return measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.tensor(max_k_indices)\n",
    "mm = m.clone()\n",
    "mm[:, 3] = 3\n",
    "m, mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_list = [10, 15, 20]\n",
    "\n",
    "for K in top_k_list:\n",
    "    top_k_ids = max_k_indices[:, :K]\n",
    "    print(f\"{top_k_ids}\")\n",
    "    # measure = ranking_evaluation(target_ids, max_k_indices, top_k_list)\n",
    "    print(f\"{K}: {Metric.hit_ratio(target_ids, top_k_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime, localtime, time\n",
    "from tqdm import tqdm\n",
    "from src.utils import Log\n",
    "from src.evaluation import Metric\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, optimizer, config):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.trainloader = DataLoader(AmazonDataset(config, 'train'), batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "        self.validloader = DataLoader(AmazonDataset(config, 'valid'), batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "        \n",
    "        # Initiating best model and best score\n",
    "        self.min_k = min(self.config['top_k_list'])\n",
    "        self.best_model = None\n",
    "        self.best_results = None\n",
    "        self.best_score = 0\n",
    "        \n",
    "        # Use f-string for better readability\n",
    "        self.log = Log(self.config)\n",
    "        self.device = torch.device(f'cuda:{config[\"gpu\"]}' if torch.cuda.is_available() else 'cpu')\n",
    "        self.metric = Metric()\n",
    "\n",
    "\n",
    "    def to_device(self, batch, device):\n",
    "        for k, v in batch.items():\n",
    "            if (\"input_ids\" in k) or ('attention_mask' in k):\n",
    "                batch[k] = v.to(device)\n",
    "            else:\n",
    "                batch[k] = v\n",
    "        return batch\n",
    "\n",
    "    def train_epoch(self, model, optimizer, loader, epoch):\n",
    "        # losses = []\n",
    "        for i, batch in enumerate(loader):\n",
    "            batch = self.to_device(batch, self.device)\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # losses.append(loss.item())\n",
    "            self.log.add(f\"Epoch: {epoch}/{self.config['max_epochs']} || Iter: {i}/{len(loader)} || Loss: {loss.item()}\")\n",
    "        \n",
    "        return model, optimizer\n",
    "\n",
    "    def evaluate(self, model, loader, metric):\n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "\n",
    "        for batch in tqdm(loader,desc=\"Evaluating\"):\n",
    "            batch = self.to_device(batch, device)\n",
    "            \n",
    "            top_k_indices, preds = model(batch, flag='valid')\n",
    "            max_k_indices = model.max_k_indices\n",
    "            target_ids = batch['target_item_id'].numpy()\n",
    "            metric.update(target_ids, max_k_indices, self.config['top_k_list'])\n",
    "        \n",
    "        return metric.get_results()\n",
    "\n",
    "    def save(self,):\n",
    "        torch.save(self.best_model.state_dict(), f\"{self.config['save_path']}/best_model.pth\")\n",
    "        self.log.add(\"Model is saved\")\n",
    "\n",
    "\n",
    "    def train_eval(self, trainloader, validloader, optimizer, config):\n",
    "        self.log.add(\"Start training\")\n",
    "        self.model.train()\n",
    "        for epoch in range(config['epochs']):\n",
    "            self.model, optimizer = self.train_epoch(self.model, self.optimizer, self.trainloader, epoch)\n",
    "            results = self.evaluate(self.model, self.validloader, self.metric)\n",
    "            self.log.add(f\"Epoch: {epoch}/{config['epochs']} || Results: {results}\")\n",
    "            \n",
    "            if results[self.min_k]['NDCG'] > self.best_score:\n",
    "                self.best_score = results[self.min_k]['NDCG']\n",
    "                self.best_model = self.model\n",
    "                self.best_results = results\n",
    "                self.save()\n",
    "\n",
    "        self.log.add(\"Best results: {self.best_results}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
